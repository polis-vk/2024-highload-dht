# Stage 2

Перед началом, я заполнил базу файлами на 2GB c помощью [PutRequest](..%2F..%2Fscripts%2FPutRequest.lua)

Запускать wrk буду со следующими параметрами: \
`wrk -d 30 -t 8 -c 64 -R <RPS> -L -s ./scripts/<ScriptName>.lua http://localhost:8080`

DaoWorkerPool имеет следующую конфигурацию: 
```
CorePoolSize = Runtime.getRuntime().availableProcessors(); (12)
MaxPoolSize = Runtime.getRuntime().availableProcessors(); (12)
Queue = ArrayBlockingQueue(256);
```

## GET
Для тестирования буду запускать [Get](../../scripts/Get2.lua) по случайному ключу.

Для начала возьму `RPS=45000` - т.е. точку разладки первого этапа.\
[get_R45k](profile_wrk%2Fget_R45k) База с лёгкостью справилась с такой нагрузкой. \
[get_R90k](profile_wrk%2Fget_R90k) Изредка начинают появляться более медленные запросы. \
[get_R110k](profile_wrk%2Fget_R110k) База не выдержала нагрузки со 110_000 запросами/сек.

Посмотрим, что будет, если уменьшить `CorePoolSize` в два раза: \
[get_lessCore_R90k](profile_wrk%2Fget_lessCore_R90k)
    - База гораздо хуже справилась с нагрузкой, потоки не успевают за селекторами. \
На профиле [get_lessCore_R90k.html](profile_html%2Fget_lessCore_R90k.html) видно, что новых потоков не создавалось
![get_lessCore_R90k.png](profile_png%2Fget_lessCore_R90k.png)
Это значит, что при нехватке ресурсов для выполнения задач, появится дополнительная нагрузка в виде создания новых потоков и
производительность станет ещё хуже, чем сейчас. \
Выставление числа потоков больше, чем доступно процессоров так же ухудшило производительность - [get_moreThreads](profile_wrk%2Fget_moreThreads)

Возвращаюсь к исходному количеству потоков и попробую уменьшить размер очереди до 64. \
[get_smallQS_R90k](profile_wrk%2Fget_smallQS_R90k) - Переполнений очереди не было, производительность осталась на том же уровне. \
Также, увеличив число соединений до 75, в логи начала выводиться информация о переполнении и в ответах начали приходить ошибки.
Остановлюсь на `QueueSize = 128`, чтобы было немного дополнительного места на случай, если один из потоков немного подвиснет.

В качестве эксперимента поменяю ArrayBlockingQueue() на LinkedBlockingQueue() \
[get_linked_queue](profile_wrk%2Fget_linked_queue) - с такой очередью производительность ухудшилась

Теперь попробою SynchronousQueue<>() \
[get_synchronous_queue](profile_wrk%2Fget_synchronous_queue) - Производительность очень плохая, половина запросов вернули ошибку.
Постоянно вылетает RejectedExecutionException из-за переполнения очереди.

Я также реализовал механизм задач с дедлайном: если между созданием задачи и взятием её на обработку проходит много времени и мы считаем, что
она уже неактуальна, задача просто отбрасывается. Потенциально это поможет сохранить пропусную способность базы в состоянии большой нагрузки, и если в какой-то
момент она подвиснет и некоторые задачи станут неактуальными, очередь быстро избавится от них и перейдёт к тем, которые ещё ждут.
Для воспроизведения такой ситуации я поставил дедлайн в 15мс и получил следующие результаты:
- С дедлайном - [get_deadline](profile_wrk%2Fget_deadline)
- Без дедлайна - [get_no_deadline](profile_wrk%2Fget_no_deadline)

Таким образом, мы сохраняем терпимую пропусную способность ценой таймаутов с некоторыми соединениями.

## Put

Для тестирования буду запускать [PutRequest](..%2F..%2Fscripts%2FPutRequest.lua).

Проверка точки разладки первого этапа (`RPS=45000`)

[put_R45k](profile_wrk%2Fput_R45k) БД выдала очень хорошее latency. \
[put_R90k](profile_wrk%2Fput_R90k) и [put_R150k](profile_wrk%2Fput_R150k) - Так же, как и у get, изредка появляются медленные запросы, но они все равно быстрее и их меньше.
Скорее всего это связано с частыми сбросами данных на диск, но, в целом, статистика приемлемая. \
[put_R160k](profile_wrk%2Fput_R160k) со 160_000 запросов в сек. база начинает не выдерживать и появляется значительная доля медленных запросов.

## Профилирование

В [аллокациях](profile_png%2FgetAlloc.png) и [цпу](profile_png%2FgetCpu.png) у GET по сравнению c первым этапом ничего особо не поменялось. \
Такая же картина и у PUT: [аллокации](profile_png%2FputAlloc.png) и [цпу](profile_png%2FputCpu.png).

Однако теперь видно, что основная логика запроса равномерно распределяется по пулу воркеров. Селекторы же теперь выполняют только парсинг и делегируют выполнение,
что отделяет обработку соединений от бизнес логики и позволяет более эффективно принимать новые соединения.

1) распределение CPU у Get по потокам
![getCpuT.png](profile_png%2FgetCpuT.png)

2) распределение CPU у Put по потокам
![putCpuT.png](profile_png%2FputCpuT.png)

Аналогичные картинки с аллокациями: [getAllocT](profile_png%2FgetAllocT.png) и [putAllocT](profile_png%2FputAllocT.png)

Что же касается блокировок, большинство из них приходятся на работу с очередями. Не думаю, что мы можем исправить это.
1) getLock
![getLock.png](profile_png%2FgetLock.png)
2) putLock
![putLock.png](profile_png%2FputLock.png)

## Дополнительно

Я также протестировал код из первого этапа с множеством соединений и получил результаты не хуже, чем у реализации с воркерами:
    
- [get_stage1](profile_wrk%2Fget_stage1)
- [put_stage1](profile_wrk%2Fput_stage1)

Так как со множество соединений приложение начинает обрабатывать множество селекторов, а не один, как в предыдущем этапе, это
так же хорошо параллелит наше приложение на данном уровне. На данный момент это даёт даже лучшие показатели, чем код второго этапа.
Думаю, это можно объяснить тем, что в многопоточной версии первого этапа мы не берём лишних блокировок на очереди и не делегируем исполнение
в другие потоки, что позволяет обрабатывать запросы быстрее. Однако в перспективе код данного этапа можно настроить так, чтобы он был ещё лучше:
пока ядро простаивает из-за чтения диска, можно передать исполнение селекторам на парсинг запроса или чтение из сокета.

## Выводы
- Наиболее оптимальной очередью для `ThreadPoolExecutor` оказалась `ArrayBlockingQueue`
- Выделение меньшего числа потоков, чем доступно процессоров, сильно замедляет работу
- Выделение потоков сверх количества процессоров тоже ухудшает производительность
- По сравнению с прошлым этапом удалось в разы поднять пропускную способность как для Get (30_000 -> 90_000), так и для
Put (30_000 -> 150_000).