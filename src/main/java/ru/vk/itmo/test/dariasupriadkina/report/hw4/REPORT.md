# Репликация

Суть данного этапа в обеспечении дублирования данных с целью повышения отказоустойчивости

Для этого мы:
1. Модифицировали алгоритм шардирования: теперь по ключу id мы детерминированно получаем несколько нод кластера
2. Ввели новые параметры ack и from, from - количество нод, на которые мы отправляем запрос, ack - количество 
необходимых ответов от нод (по умолчанию: from - число нод в кластере, ack - кворум от числа нод в кластере)
3. Мержим результаты, полученные от нод, решаем конфликты в случае их расхождений, по средствам сравнения таймстампов 

В прошлой лабораторной работе, при реализации шардирования, несмотря на то, что бин-поисков при запросе на получение 
стало меньше (так как уменьшилось количество sstable'ов, хранимых на одной ноде), сервер стал деградировать и выдерживать 
меньшую нагрузку по сравнению с сервером, не поддерживающим шардирование.  Тогда это произошло из-за добавившейся сетевой 
нагрузки между нодами. Сейчас, с появлением реплик, сетевого взаимодействия должно стать больше, да и количество хранимых 
данных на одной ноде за счет дублирования данных должно увеличиться, а значит и количество sstable'ов, а значит и 
бин-поисков... То есть, в очередной раз стоит ожидать "отрицательный результат" с точки зрения выдерживаемого rps, 
вопрос лишь насколько

NB! Все эксперименты производились с дефолтными значениями ack и from для кластера из 3х нод, то есть ack=2, from=3

## Нагрузочное тестирование

### PUT

Точка разладки наблюдается на ≈12000rps

60s
```
wrk -d 60 -t 1 -c 64 -R 12000  -L -s /Users/dariasupriadkina/IdeaProjects/2024-highload-dht/src/main/java/ru/vk/itmo/test/dariasupriadkina/scripts/upsert.lua http://localhost:8080 
Running 1m test @ http://localhost:8080
  1 threads and 64 connections
  Thread calibration: mean lat.: 40.533ms, rate sampling interval: 217ms
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency    37.17ms   42.09ms 172.93ms   79.74%
    Req/Sec    12.05k     3.67k   19.49k    57.89%
  Latency Distribution (HdrHistogram - Recorded Latency)
 50.000%   15.66ms
 75.000%   68.86ms
 90.000%  104.38ms
 99.000%  143.36ms
 99.900%  164.99ms
 99.990%  169.47ms
 99.999%  172.29ms
100.000%  173.06ms
```

120s
```
 wrk -d 120 -t 1 -c 64 -R 12000 -L -s /Users/dariasupriadkina/IdeaProjects/2024-highload-dht/src/main/java/ru/vk/itmo/test/dariasupriadkina/scripts/upsert.lua http://localhost:8080
Running 2m test @ http://localhost:8080
  1 threads and 64 connections
  Thread calibration: mean lat.: 46.278ms, rate sampling interval: 229ms
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency    32.84ms   40.54ms 174.34ms   80.10%
    Req/Sec    12.03k     3.59k   19.89k    61.72%
  Latency Distribution (HdrHistogram - Recorded Latency)
 50.000%    9.21ms
 75.000%   60.99ms
 90.000%  100.67ms
 99.000%  136.96ms
 99.900%  160.77ms
 99.990%  169.98ms
 99.999%  172.67ms
100.000%  174.46ms

```

На прошлом этапе мы могли держать около 25000rps. Разница в ≈2 раза. 

### GET (RANDOM)

!Измерения происходят на предварительно заполненной базе (≈3500000 записей)

Точка разладки наблюдается на ≈16000rps

60s
```
wrk -d 60 -t 1 -c 64 -R 16000 -L -s /Users/dariasupriadkina/IdeaProjects/2024-highload-dht/src/main/java/ru/vk/itmo/test/dariasupriadkina/scripts/getrand.lua http://localhost:8080
Running 1m test @ http://localhost:8080
  1 threads and 64 connections
  Thread calibration: mean lat.: 3.901ms, rate sampling interval: 20ms
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency     3.98ms    3.86ms  38.53ms   82.57%
    Req/Sec    16.40k     3.93k   25.68k    69.32%
  Latency Distribution (HdrHistogram - Recorded Latency)
 50.000%    2.25ms
 75.000%    5.24ms
 90.000%   10.35ms
 99.000%   15.47ms
 99.900%   23.15ms
 99.990%   36.00ms
 99.999%   37.44ms
100.000%   38.56ms
```

120s
```
wrk -d 120 -t 1 -c 64 -R 16000 -L -s /Users/dariasupriadkina/IdeaProjects/2024-highload-dht/src/main/java/ru/vk/itmo/test/dariasupriadkina/scripts/getrand.lua http://localhost:8080
Running 2m test @ http://localhost:8080
  1 threads and 64 connections
  Thread calibration: mean lat.: 3.923ms, rate sampling interval: 20ms
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency     3.85ms    3.75ms  32.80ms   82.77%
    Req/Sec    16.40k     3.83k   26.00k    68.77%
  Latency Distribution (HdrHistogram - Recorded Latency)
 50.000%    2.20ms
 75.000%    4.95ms
 90.000%   10.01ms
 99.000%   15.44ms
 99.900%   23.17ms
 99.990%   29.15ms
 99.999%   31.86ms
100.000%   32.83ms
```
На прошлом этапе мы могли держать около 33000rps. Разница также в ≈2 раза.

## Профилирование

Профилирование запускаем:
- PUT - 9000rps
- GET (random) - 12000rps

### PUT

#### CPU

![upsert-cpu.png](screenshots%2Fupsert-cpu.png)

В целом, профиль очень похож на профиль предыдущей лабораторной работы
В процентном соотношении практически ничего не поменялось
Добавились новые методы, но они встречаются в совсем небольшом количестве семплов:

- mergeResponses - 0.58%
- getNodesById - 0.94% (раньше был метод getNodeById, он занимал 0,78. Увеличение может быть вызвано тем, что мы теперь 
не просто выбираем максимум, а условно сортируем с выбором определенного количества нод)
- getFromAndAck - 0.45%

Эти методы вряд ли сильно могли повлиять на нагрузку в целом, поэтому особого смысла в их оптимизации я не вижу


На профилях также хорошо видно, что upsert теперь у нас происходит в нескольких местах: 
в случае, если у нас запрос пришел от пользователя, мы не имеем заголовка X-FROM, поэтому осуществляем broadcast
(рассылку по нодам, которые были получены в ходе выполнения метода getNodesById), если же этот заголовок уже установлен,
значит, мы понимаем, что запрос пришел от другой ноды, а следовательно, предполагается, что значение по данному ключу 
находится у нас, поэтому - отправляем на своего обработчика

![upsert-duo.png](screenshots%2Fupsert-duo.png)

#### ALLOC

![upsert-alloc.png](screenshots%2Fupsert-alloc.png)

Пофили также выглядят весьма похожим образом, однако стоит отметить:
- Увеличились аллокации в методе handleRequest (было около 22%, стало около 30%), который мы в значительной степени переписали,
отказавшись от эталонной реализации для обеспечения репликации
  - Новый метод broadcast по своей сути похож на старые хендлеры, поэтому по памяти он и занял примерно столько же - 18,35%
    (в прошлой лабораторной putHandler занял - 18,36% аллокаций), к нему добавились новые методы - getFromAndAck - 0.56%, 
    mergeResponses - 2.57%, getNodesById - 4,44%, отсюда и появились лишние аллокации. Возможно имеет смысл все таки рассмотреть 
    способы оптимизации с точки зрения памяти этих методов

#### LOCK

![upsert-lock.png](screenshots%2Fupsert-lock.png)

С точки зрения локов, ничего как будто бы вообще не изменилось, профили с предыдущей лабой практически идентичны.
Все методы в процентном соотношении отличаются максимум на 1%

### GET

#### CPU

![getrand-cpu.png](screenshots%2Fgetrand-cpu.png)

Несмотря на мое предположение в начале о том, что из-за увеличения общего объема данных и => увеличения количества sstable'ов  
на каждой ноде, мы должны были получить больше бин-поисков => больше затрат CPU, но этого не произошло. У меня есть предположение, 
что по сравнению с прошлым разом текущая база данных сама по себе содержит меньшее количество данных 
(всего в районе 3_500_000, в прошлый раз было в районе 5_000_000)

Также в случае перессылки данных мы можем осуществлять в рамках broadcast 2 бинпоиска (так как нод в нашем эксперименте - 3, а кворум, следовательно - 2), 
отсюда и следует "2 горб", где затрат на бинпоиск в ≈2 раза больше

Из-за этих идентичных "горбов" затраты на handleRequest выросли с 23% до 44%, что вероятно и привело к такой сильной деградации сервера


#### ALLOC

![getrand-alloc.png](screenshots%2Fgetrand-alloc.png)

С точки зрения аллокаций, все абсолютно аналогично PUT-запросам: вырос handleRequest (с 23% до 30%)

#### LOCK

![getrand-lock.png](screenshots%2Fgetrand-lock.png)

Аллокация также выглядят практически идентично. Единственное, что немного отличается - метод ThreadPoolExecutor.getTask. 
Процент локов вырос с 8% до 11%. Возможно это также связано с увеличением затраченного времени на реализацию текущей бизнес логики
(по сравнению с прошлым этапом мы как минимум добавили шаг с перессылкой данных на другие ноды), из-за чего очередь наполняется медленнее.


## Выводы и возможные улучшения

Репликация позволила нам повысить доступность данных. Теперь при отказе какой-либо ноды мы не теряем доступ к данным, 
которые расположены на этой ноде 
(ну или, по крайней мере, не ко всем данным, все зависит от того, какой ack указал пользователь). Однако, ради этой 
доступности нам пришлось пожертвовать производительностью, которая ухудшилась в среднем в ≈2 раза.


Кажется, что в данной лабораторной работе есть места, которые еще можно распараллелить

Например, сейчас мы, чтобы достичь кворума отправляем запросы на другие ноды последовательно, из-за чего нам приходится 
ждать, когда каждая из нод получит запрос, выполнит удаление/добавление/поиск и ответит (то есть бизнес-логика + сетевуха), что в купе 
достаточно большие затраты по времени. Выглядит так, что нам ничего не мешает работать с каждой нодой в отдельном потоке, ожидая, 
пока не будет получен кворум ответов. Правда, уверена, что и здесь будут свои подводные камни, например, когда мы получили кворум, 
нам уже не нужены ответы других потоков, а так или иначе они были запущены и работают. Получается, что мы определенным 
образом потратили ресурсы впустую.