Оглавление:
 - [First stage.](#First-stage.)
 - [Этап Второй](#Этап-Второй.) 
 - [Этап Третий](#Этап-Третий.)

# First stage.

Итак в этом этапе необходимо: 
 - было найти точки разладки для операций put и get.
 - профильнуть для put и get при нагрузке

Компу на котором проводится тестирование уже ~12 лет. 

Точка разладки для put лежит где-то в диапазоне от 8к-11к.
Если обратить внимение на RPC, то в 2х замерах у меня он приближен к тому, который и должен быть.

[первый замер](./results/loading_with_put_good.txt)
[второй замер](./results/loading_with_put_bad1.txt)

Но вот если посмотреть на время отклика, на 95 персентиль, к примеру.
Уже будет другая картина.

Ответ от сервера длиной в 611.327ms звучит очень грустно, и уже говорит нам, что что-то пошло не так.
В свою очередь, при втором замере 172ms может и хорошее время, смотря что мы считаем хорошим;)

Поэтому могу сделать вывод, что точка разладки put находится между 2мя приведенными замерами.

Точка разладки для get лежит от в диапазоне от 4к до 6к. Перед get запускал скрипт, чтобы база проинициализировалась.

[первый замер](./results/loading_with_get_good.txt)
[второй замер](./results/loading_with_get_bad.txt)

## Профилирование.
1 - PUT

[ALLOC](./results/allocput.html) - аллокация была при записи SSTable'ов в память. Аж 12%. 
Ну чтож, возможно, стоит рассмотреть другой вид хранения данных. Или как-то сжимать данные перед записью 

Но со сжатием в любом случае надо быть аккуратным, так как по асимптотике можем просесть сильно.

Вторая по частоте(7%) была sendResponse->Response.toBytes.

На мой взгляд это некая сериализация данных перед отправкой, поэтому особо это пофиксить не получиться. Кроме как изменить способ передачи данных и отказаться от http))

[CPU](./results/cpuput.html) - 20% было потрачено на readRequest. То есть cpu простаивал в режиме ожидания новых запросов и чтения их. 


2 - GET

[ALLOC](./results/allocget.html) - сложно выделить место, где однозначно больше всего аллоков, она в среднем занмают по 10%.

[CPU](./results/cpuget.html) - аж 60% происходило сравнение memsegm'ов. Есть смысл, возможно, добавить эвристику для сравния или распаралеллить эту задачу.
 
# Этап Второй.

Для начала разберемся с настройками wrk. Далее особо их менять не буду, кроме latency.

-c 64 - чтобы не было ограничений со стороны отправки пакетов.

-t 4 - представим, что одновременно работают с нашей dao сразу несколько человек. Для очереди полезно будет узнать.

Первое замечание, которое стоит упомянуть, что следует выставлять время жизни idle тредов не супер большим.
Пару замеров wrk при увеличении latency и повторной прогонки дали понять, что увеличение keepAliveTime приведет к незначительной деградации.

### MaxTheadSize

Всего у меня 8 ядер старого процессора. Поэтому попробую подобрать разные максимальные значения потоков для нашего экзекутора.

![maxtheradsize](results/maxthreadsize.png)

Особой разницы как видим нет. Обращать внимание на 99.999+ смысла особого нет, наверняка затесались выбросы.

Но есть момент, когда при 8ми идет лучше. Остановимся на этом. Брать выше 8ми не имеет смысла, так как система попросту не сможет позволить себе больше настоящих тредов.
(те 8, скорее всего, были достигнуты Hyper threading'ом).

Теперь дошли до самого интересного - выбора самой блокирующей очереди.

### LinkedBlocking, LinkedTransfer, ArrayBlocking

![3Queues](results/3QueuesNorm.png)

Был опыт, когда не передал capacity и linkedBlocking улетел в небеса. Поэтому важно понастраивать capacity.
У linkedtransfer его нет, поэтому смысла его рассматривать нет.

Так, теперь более наглядный график.

![2Queue](results/2Queues.png)

Ответы сервера происходили не за экстремально большое время, поэтому смысл посравнивать их между собой их есть.


Разные параметры capacity.

capacity == 50.

![50Cap](results/50cap.png)

capacity == 256

![256Cap](results/256cap.png)

видно из графиков, что для arrayblocking нужно увеличивать cap, а c linkedBlocking иначе.

По исходному коду linkedblocking видны места, где написаны условия 

```
c = count.getAndIncrement();

if (c + 1 < capacity)
    notFull.signal();  // <-- lock
```
следовательно, имеем в виду, что нужно под это подстраиваться (нельзя слишком много, иначе время отклика будет бесконечно большим и слишком маленьким, чтобы постоянно срабатывал if) 

Пусть дальше для сравнений по cpu, alloc и lock далее будет использоваться оптимальное capacity, чтобы они выдавали примерно одно и то же.


![sim](results/simillar.png)

### CPU, ALLOC, LOCK

#### CPU

[Array](./results/CPUArrayBlocking.html)
[Linked](./results/CPULinked.html)

11% - array, 8% - linked. Как будто разница такая из-за различных форм await(awaitNanos - linked(5%), await - array(8%)). Как раз те самые 3 процента.
Ожиданий в arrayblocking больше.

#### ALLOC
[Array](./results/ALLOCArrayBlocking.html)
[Linked](./results/ALLOCLinked.html)

Особой разницы нет, кроме того, что в linkedblocking встречаются rejectedExecution.
Связано с тем, что в сорс коде екзекутора eсть место с queue.poll(keepAliveTime, elem). Именно тут идет потеря пакетов, видимо связанная с неэффективностью linkedblocking.


#### LOCK
[Array](./results/LOCKArrayBlocking.html)
[Linked](./results/LOCKLinked.html)

Почти одинакового. У Array это метод ArrayBlocking.take, у linked - LinkedBlokingQueue.poll.

Думаю, что связано это как-то с тем, что в исходниках в основном все блокировки брались на все функции целиком, поэтому особой разницы и не видно.
Никакой измеренной логики ни там, ни там не было использовано.

GET запросы

Перед каждым get был put, чтобы база была непустой. И перед каждым замером перезапускал сервер, чтобы 
не было ничего в кэше. И нагрузки выдерживали в разы меньшие, пришлось снизить общее кол-во запросов.


#### CPU

[Array](./results/CPUArrayGET.html)
[Linked](./results/CPULinkedGET1.html)

Смотрим на take у обоих реализаций. Кол-венно их примерно одинаково. Вот только с mismatch получились разные результаты. 
Дополнительно перезамерил и увидел, что у linked не очень стабильно кол-во вызовов этой функции. 
Вот еще [пример](./results/CPULinkedGET.html). Как это связано с очередью, если это по сути работа с dao, которое одно и то же...


#### ALLOC 

[Array](./results/ALLOCArrayGET.html)
[Linked](./results/ALLOCLinkedGET.html)

Картины по аллокациям очень близкие, сложно выделить существенную разницу.

#### LOCK

[Array](./results/LOCKArrayGET.html)
[Linked](./results/LOCKLinkedGET.html)

Раншье лок был примрено одинаковый, теперь на 20% локов больше со стороны LinkedBlocking.poll.

Смотрим по абсолютным значениям вызовов. await и awaitNanos в нашей задаче отработали очень близко, а вот lockInterruptibly в случае linked срабатывал аж в 4ре раза больше, чем у array.

Скорее всего, это связано с timeout. Раньше в пут при одинаковых бОльших нагрузках были ответы от сервера > 300. То есть они выходили раньше из poll и лока уже не было.
Сейчас он меньшее кол-во раз выходил с return null, то есть больше находился под локом.

Итого: среди всех очередей на первом месте оказался ArrayBlockingQueue. 

Правило, если не знаешь что брать из структур данных, то возми ту, что на динамическом массиве работает;) (по аналогии со списками)


# Этап Третий.

Вижу смысл проделать такие же замеры, что и были в первом этапе,
так как по существу могла изменится точка разладки,
да и перераспределится серверная нагрузка, появлились дополнительные расходы на переотправку, нахождение нужной ноды.

Итак, начнем с опрделения точки разладки.

пусть возьмем 
-c = 64
-t = 4 

С прошлого этапа(все равно мы не убрали свою очередь, почему бы в несколько потоков не протестить)
Запуск на 11к дает очень хорошие результаты(ну их фактически нельзя полноценно сравнить с первым этапом, потому что изменилось -t -c)
Тем не менее результат [хороший](/ru/vk/itmo/test/kachmareugene/results/task3/old11kput.txt).

Точка разладки теперь 
находится между [60к](/ru/vk/itmo/test/kachmareugene/results/task3/new60kput.txt) 
и [70к](/ru/vk/itmo/test/kachmareugene/results/task3/new70kput.txt).

Для get точка разладки находится между [14к](/ru/vk/itmo/test/kachmareugene/results/task3/new14kget.txt) 
и [14.5к](/ru/vk/itmo/test/kachmareugene/results/task3/new14.5kget.txt).

## CPU

У get ничего сильно не изменилось, все также основную долю времени занимает поиск ключа и сравение memsegment.

В свою очередь соразмерно с вызовами обработчика put запросов вызывался client.invoke() для переправки на другую ноду.


## ALLOC

У put и get чересчур много аллокаций вызванных с client'ом появилось. Наверняка, это, скорее всего, бага и можно улучшить поведение


# Четвертый этап

В этом этапе нагрузка в виде wrk помогла мне обнаружить существенную багу в коде, которую не покрывали тесты.


Дело в том, что я, когда определяю остальные 'slave' ноды 
я могу добавить и 'master' к общему числу нод, 
которым я собирался отослать redirect для репликаций.

И всего одну строчку нужно было изменить в методе responseSafeAdd в HttpSeverImpl, чтобы не слались запросы на ту же ноду.

## Точка разладки

Первым делом я запустил wrk на [60к](results/task4/put60k.txt) для put.
Аж в 1800 раз стало хуже производительность, судя по персентилям.

Но на мой взгляд не стоит смотреть на такую статистику, 
так как при 60к находится уже за областью точки разладки. 

Если снизить rps и посмотреть на меньшую нагрузку, обнаружим, что при [34к](results/task4/put34k.txt)
уже нормальные персентили. То есть можно считать, что возможности сервера для put запросов спали в ~2 раза.

Get [14к](results/task4/get14k.txt) держалась нагрузка
очень хорошо, всего в 4ре раза стали хуже персентили по сравнению с прошлым этапом.


И это все равно хороший результат, сильной деградации в get не произошло. 

Такое 'проседение' в производительности безусловно обозначено необходиомостью  


## CPU 

[PUT](results/task4/putCPU.html).

Кол-во использования редиректа увеличилось с 500 sampl'ов, до 2000 тысяч.
(HttpClient.invoke) потому как приходилось использовать пересылку на другие ноды практически всегда.

Так же можно это пронаблюдать с вызовом HttpSession.sendResponse. В 4ой лабе эти вызвовы занимают аж 35% по сравнению с 4% в 3ей лабе.


[GET](results/task4/getCPU.html).

Схожая ситуация. 19% отнимает работа с сессиями и редиректом. Всего 18% процентов из-за того, что я запускал на rps = 14к, вместо 34k, как на put.
В то же время, в прошлой лабе эти вызовы занимали eps процессорного времени.


## ALLOC

[PUT](results/task4/putALLOC.html)

На 4ом этапе появилось необходимость 
в поиске сразу нескольких урлов
PartitionMetaInfo.getSlaveUrls.

И это вызвало дополнительную нагрузку на выделение памяти.
Без такого метода не обойтись. В следующих этапах, вижу осмысленным убрать преобразование к листу, чтобы было меньше аллокаций.


[GET](results/task4/getALLOC.html)

Аналогично, с предыдущим возникла новая нагрузка в виде PartitionMetaInfo.getSlaveUrls.
Еще вижу, что увеличилось использование HttpSession.parseRequest. Но опять же это все связано из-за пересылки на другие ноды.


## LOCK

[PUT](results/task4/putLOCK.html)
[GET](results/task4/getLOCK.html)

Если сравнивать с прошлыми замерами по блокировкам,
то ничего нового не появилось.
Оно и понятно,
нигде логику с доп многопоточными структурами мы не добавляли.

Только видоизменилось немного процентное соотношение некоторых вызовов, но это связано скорее с тем, что была запущена немного большая или меньшая нагрузка.

### Вывод

На данном этапе была повышена надежность нашей распределенной системы, 
но вместо этого мы заплатили производительностью.

Теперь стал put медленней примерно в 2 раза, а get немного ухудшил свои персентили но не критично.



