## 1. Тестирование с wrk2

**Порядок действий**:
1. Запускаю сервер через `./gradlew run`
2. Запускаю wrk и с выбранным rps.
3. Выключаю сервер.
3. Смотрю перцентиль.
4. Примерно понимаю на каком этапе стало снижаться latency и сужаю область поиска точки разладки, условный бинпоиск.
5. Возвращаюсь на этап 1 пока не найду точку

### 1.1 Get

**Команда для запуска:**
```
 wrk -d 30 -t 1 -c 1 -R 50000 -L -s ./src/main/java/ru/vk/itmo/test/kovalchukvladislav/resources/lua/get.lua http://localhost:8080 >> src/main/java/ru/vk/itmo/test/kovalchukvladislav/resources/wrk/get50000
```


**Итерации:**
1. Запуск с 50000 rps. latency почти сразу начинает проседать, где-то с 20-30 персентиля. График шумный, не понятно.  
[get50000](wrk/get50000)  
[get50000.png](wrk/get50000.png)
2. Запуск с 35000 rps. График стал менее шумным, но все еще не понятным. Так же, где то с +- 30% перцентиля все растет.  
[get35000](wrk/get35000)  
[get35000.png](wrk/get35000.png)  
3. Запуск с 20000 rps. Намного лучше, рост идет после 92 перцентиля.  
   [get20000](wrk/get20000)  
   [get20000.png](wrk/get20000.png)
4. Запуск с 17000 rps. Ожидалось, что рост пойдет позже 92 перцентиля, но он примерно там же: 85-90.  
   [get17000](wrk/get17000)  
   [get17000.png](wrk/get17000.png)
5. Запуск с 15000 rps. Идеально. Высокие значения на 99.3+ перцентиле. Можно сказать что это точка разладки (но скорее она выше, между 15к и 17к)  
   [get15000](wrk/get15000)  
   [get15000.png](wrk/get15000.png)
6. Запуск с 10000 rps. Лучше, чем в 15000, но совсем чу-чуть.  
   [get10000](wrk/get10000)  
   [get10000.png](wrk/get10000.png)

**Вывод**:
Наш сервер уверенно выдерживает 10k get запросов в секунду. Точка разладки находится в 15k rps (даже выше, между 15к и 17к).

Тут происходит упор в CPU, так как диск мы не нагружаем и ничего не пишем.
Либо это межсетевое (в данном случае локальное) взаимодействие.

### 1.2 Put

**Команда для запуска:**
```
wrk -d 30 -t 1 -c 1 -R 50000 -L -s ./src/main/java/ru/vk/itmo/test/kovalchukvladislav/resources/lua/put.lua http://localhost:8080 >> src/main/java/ru/vk/itmo/test/kovalchukvladislav/resources/wrk/put50000
```

1. Запуск с 50000 rps. У 50000 put намного позже растет latency, чем у 50000 get. Примерно 60-80 перцентиль.
[put50000](wrk/put50000)  
[put50000.png](wrk/put50000.png)

2. Запуск с 35000 rps. График менее шумный, двигаемся к +- 75 перцентилю.  
[put35000](wrk/put35000)  
[put35000.png](wrk/put35000.png)

3. Запуск с 26000 rps. Рост на 98.9 перцентиле. Можно сказать это точка разладки.  
[put26000](wrk/put26000)  
[put26000.png](wrk/put26000.png)

3. Запуск с 25000 rps. Идеально! Рост на 99.7 перцентиле.  
[put25000](wrk/put25000)  
[put25000.png](wrk/put25000.png)


**Вывод**:
   Наш сервер уверенно выдерживает 25к put запросов в секунду. Точка разладки в 26к (даже с запасом, на самом деле между 26к и 35к).  
   Почему именно такое число? Тут происходит упор в диск и фоновый flush.

   Примерно посчитаем размер всех данных. Я пишу пару `k(number):v(number)`. Посчитаем длину в байтах строки `k25000`.  
   Это 6 байт. Еще 6 байт на значение. Еще по 8 байтов на 2 long'а для их длины. В сумме примерно 20 байтов.
   20 байтов умножить на 26к запросов в секунду = 520000 байт = примерно 500Кб. То есть каждые 2 секунды мы делаем flush()

   С этим наш сервер без проблем справляется и поддерживает примерно тот же latency.
   Но если увеличить rps, flush будет работать чаще, и latency начинает сильно отклоняться.
   
   Можно увеличить flush threshold, и тогда flush() будет происходить реже.
   Но и это делать надо аккуратно: как мы помним из предыдущего пункта, на 15000 get запросов мы уже упираемся в CPU (или сеть).
   Поиск оптимального flush threshold зависит, в том числе от производительности CPU и сколько мы готовы хранить в памяти.


UPDATE: Подумал, что для совсем точности мне следовало сделать ключи и значения одинаковой длины. 
Чтобы не было, что первые запросы имеют пару `k1:v1`, а последние `k25000:v25000`.
К сожалению, не успею до дедлайна все перестировать. График перцентилей был бы лучше, так как конечно на последних запросах надо больше памяти и производительности, чем на первых.  
Если проверяющий даст добро, то я готов :)

## 2. Профилирование с async-profiler

Для тестирования возьму rps ниже точки, которые вычислил в первом тесте: 10к get и 25k put.  
Профилировать get с пустой бд бессмысленно -- код dao занимает меньше 1% времени, я проверял.

Поэтому тестирование будет выглядеть вот так:
1. Запускаем сервис
2. Делаем put ключей `k1:v1 ... k25000:v25000`. Холодный старт.
3. Делаем put ключей `k1:v1 ... k25000:v25000`. Горячий старт.
4. Делаем get ключей `k1:v1 ... k10000:v10000`. Холодный старт.
5. Делаем get ключей `k1:v1 ... k10000:v10000`. Горячий старт.
6. Выключаем сервис.

На этапах 2-5 включаем/выключаем профайлер.  
Этот цикл пришлось делать два раза: сначала для cpu, потом для alloc.  
В async-profiler есть возможность посмотреть сразу обе метрики, но оно несовместимо с flamegraph.
> [ERROR] Only JFR output supports multiple events

Почему я решил делать так:
* Get запросы теперь будут тратить достаточно CPU
* Мне очень интересно посмотреть на разницу между холодным и горячим стартом.

Это профилирование не идеально для поиска разницы горячего/холодного старта, так как на 3м этапе мы добавляем дубликаты.  
Размер dao до операций на этапах 2 и 3 отличается в два раза. Но все же интересно :)

**Пример команды для запуска**
```

./gradlew run

[Повторить 4 раза с разными параметрами]
asprof start -e alloc -o flamegraph -f src/main/java/ru/vk/itmo/test/kovalchukvladislav/resources/asprof/memory_put25000_cold.html 3938 

wrk -d 30 -t 1 -c 1 -R 25000 -L -s src/main/java/ru/vk/itmo/test/kovalchukvladislav/resources/lua/put.lua http://localhost:8080 >> src/main/java/ru/vk/itmo/test/kovalchukvladislav/resources/wrk/put25000_cold_memory 

asprof stop -e alloc -o flamegraph -f src/main/java/ru/vk/itmo/test/kovalchukvladislav/resources/asprof/memory_put25000_cold.html 3938
[\Повторить 4 раза]

./gradlew --stop
```

(Еще я не понял как html которую выдает profiler в svg конвертировать, сори)

### 2.1 CPU

**1. PUT 25000. Холодный старт**

[cpu_put25000_cold.html](asprof/cpu/cpu_put25000_cold.html)  
[cpu_put25000_cold.png](asprof/cpu/cpu_put25000_cold.png)

Если сделать поиск по regex моего пакета (`*kovalchuk*`), фиолетовым на png'шке покрашены классы моего пакета.  

Matched 8.99% всей загруженности CPU:
* 1.25% забирает `flush()`
* 5% `DaoHttpRequestHandler` (интересно как). Но из моего тут только хендлер, все остальное это `one-nio` под капотом
* 2.72% `putEntity`, из которых много расходов на сравнение `MemorySegment`'ов компаратором.

Почти все забирает на себя one-nio.

**2. PUT 25000. Горячий старт**

[cpu_put25000_hot.html](asprof/cpu/cpu_put25000_hot.html)  
[cpu_put25000_hot.png](asprof/cpu/cpu_put25000_hot.png)

Matched 7.63 %:
* `putEntity` теперь 4.92%
* `flush` теперь 2.71%

DaoHttpRequestHandler. Тут мы видим что изначально в dao уже было 25к записей, когда в холодном старте не было. 
Поэтому CPU usage по этим методам и выросло.  

Есть и приятные моменты: thread_start упал с 14.73% до 2.54%.

**3. GET 10000. Холодный старт**

[cpu_get10000_cold.html](asprof/cpu/cpu_get10000_cold.html)  
[cpu_get10000_cold.png](asprof/cpu/cpu_get10000_cold.png)

Matched 18.76%, все уходит на getEntity. Внутри него:
* 16.82% на бинпоиск в SSTable: 11.01 % на сравнение MemorySegment'ов и 3.47% на чтение.
* 0.82% на сравнение сегметов в InMemoryDao.

one-nio и thread'ы кушают столько же, сколько на предыдущем этапе.

**4. GET 10000. Горячий старт**

[cpu_get10000_hot.html](asprof/cpu/cpu_get10000_hot.html)  
[cpu_get10000_hot.png](asprof/cpu/cpu_get10000_hot.png)

Matched 31.14%. Все та же история, все забирает бинпоиск и сравнение.  
Из приятного: больше не создаются новые треды, а берутся из очереди.  
Однако я открыл вывод wrk2, и увидел что время по сравнению с холодным не изменилось, а рост latency с повышением перцентиля уменьшился.
Также уменьшилась максимальная latency.  

Значит, все сэкономленные ресурсы просто переназначились функциям dao.
Время не изменилось, потому что сервер может выдержать такую нагрузку. Было бы интересно потестить с большим количеством запросов.
Мне кажется что горячий старт отработал бы быстрее, учитывая что максимальная время существенно упало (3.81ms -> 3.34 ms с percentile 100%)

**Вывод по CPU:**
Горячий старт явно лучше холодного, подтверждается выводом wgk.  
Также, большинство ресурсов сейчас забирает one-nio. Но если запросы можно распараллелить и распределить, то сделать такое с бд непросто.  
Значит стоит искать возможности оптимизации dao, в частности, его основных методов которые нужны и вызываются везде:
1. Сравнение MemorySegment'ов и бинпоиск по файлу на диске.
2. Подбор оптимального значения flushThreshold с учетом особенностей CPU, Memory, Storage.
3. Ну и конечно можно делать распределенную систему.

### 2.1 ALLOC

**1. PUT 25000. Холодный старт**

[memory_put25000_cold.html](asprof/memory/memory_put25000_cold.html)  
[memory_put25000_cold.png](asprof/memory/memory_put25000_cold.png)

Matched 33.92%: 0.33% при флаше и 33.59% при putEntity.
1. Из putEntity 8.35% и 7.70% это конвертация из строки в MemorySegment. С этим ничего не можем поделать.
2. 4.24% это создание новой node'ы при вставке в InMemoryDao.
3. 8.02% это создание Response'а. Тут я понял что в коде есть лишние аллокации, и там можно сделать статическое поле.


**2. PUT 25000. Горячий старт**

[memory_put25000_hot.html](asprof/memory/memory_put25000_hot.html)  
[memory_put25000_hot.png](asprof/memory/memory_put25000_hot.png)

Matched 31.26%. 0.37% при флаше и 31.26% при putEntity.  
Все точно также как в холодном старте, просто небольшое перераспределение всего.

**3. GET 10000. Холодный старт**

[memory_get10000_cold.html](asprof/memory/memory_get10000_cold.html)  
[memory_get10000_cold.png](asprof/memory/memory_get10000_cold.png)

Matched 94.69%. Все это аллокации при бинпоиске в SSTable, так как данных много, и они зафлашились на диск.  
Почти все (90.42%) это asSliceNoCheck.

**4. GET 10000. Горячий старт**

[memory_get10000_hot.html](asprof/memory/memory_get10000_hot.html)  
[memory_get10000_hot.png](asprof/memory/memory_get10000_hot.png)

Matched 95.13%. Все точно также как в холодном старте.

**Вывод по Alloc:**
1. С точки зрения аллокаций горячий старт почти ничем не отличается от холодного. На самом деле ожидаемо.  
Однако, по логам wrk, в get в горячем старте наблюдается снижение максимального latency: 3.81 ms -> 2.96 ms.
Это сработал кеш.
В upsert() я разницы не увидел.

2. Видны моменты, которые нуждаются в оптимизации. Как минимум, огромное количество asSlice'ов (90% в get) и 8% response (в put).
И если над первым надо подумать, то второе можно пофиксить в одну строчку. 
Я решил не менять так как 
a) делаю перед дедлайном и нет времени опять все тестировать
б) так намного интереснее, ведь цель как раз таки была увидеть неоптимизированные моменты. Было бы скучно все пофиксить и написать как у меня хорошо :)

