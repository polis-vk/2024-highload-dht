## Факты

Была обновлена система с Monterey на Sonora, а также почищен диск

В связи с чем результаты значительно выросли,
особеннно в области **GET** запросов, которые очень много работают с диском

Получается достаточное количества места на диске важный параметр

Либо стоит благословить Купертино

## Нагрузка
### Характеристики
Macbook Pro M1 16GB после перезапуска


## Старый код
До этого мы тестировали при 1 подключении и 1 потоке

Соответственно у нас было 2 ограничения:
1. Мы могли упираться в производительность wrk2
2. Наш код работал только в одном потоке, несмотря на количество селекторов, т.к. подключение 1

Поэтому перезапустим старый код и найдем новые rps
### PUT

По [локам](old_base_lock.html) переодически можно заметить flush, который тормозит селекторы

Как мы видим [15000](old_base_wrk.txt) нагрузка настолько слабая,
что нам не понадобились даже все селекторы (их только 4)
[cpu](old_base_cpu.html)

Немного поискав
* [30000](old_30000_wrk.txt)
* [60000](old_60000_wrk.txt)
* [52000](old_52000_wrk.txt)

остановился на [48000](old_stable_wrk.txt) RPS

[lock](old_stable_lock.html) из интересного можно заметить,
что по окончании работы закрывается множество сессий

информация об этом пишется в лог и можно сделать выводы о том, что:
1. не стоить злоупотреблять логами
2. по этому столбику в целом можно смотреть о проблемности остальных локов
   *(чем он больше, тем менее значимы остальные локи)*

По [cpu](old_stable_cpu.html) видно, что большая часть ресурсов уходит на обработку подключений

5 min: [rps](old_stable_long_wrk.txt),
[alloc](old_stable_long_alloc.html)
[cpu](old_stable_long_cpu.html),
[lock](old_stable_long_lock.html)

Как мы видим просадки теперь намного значительнее, т.к. flush блокирует все потоки

### GET

Немного поискав
[60000](old_get_60000_wrk.txt)
[80000](old_get_80000_wrk.txt)
[85000](old_get_85000_wrk.txt)
[90000](old_get_90000_wrk.txt)
[100000](old_get_100000_wrk.txt)

Остановился на 80000 rps
5 min:
[alloc](old_get_stable_long_alloc.html)
[cpu](old_get_stable_long_cpu.html)
[lock](old_get_stable_long_lock.html)
[rps](old_get_stable_long_wrk.txt)

Как мы видим из [lock](old_get_stable_long_lock.html) никаких локов на гет запросах нет, как и из кода

В целом ничего, кроме огромного роста производительности отметить нечего

И если в случае с **PUT** запросами он был всего в несколько раз,
то здесь рост достаточно линеен, что может говорить о высоком параллелизме кода

## Новый код

### ArrayBlockingQueue

Производительность только упала

#### PUT

Как показали опыты: poolSize имеет значение.
* 16:
* * 43000: [alloc](new_put_a_16_43000_alloc.html),
    [cpu](new_put_a_16_43000_cpu.html),
    [lock](new_put_a_16_43000_lock.html),
    [rps](new_put_a_16_43000_wrk.txt)
* * 48000: [alloc](new_put_a_16_48000_alloc.html),
    [cpu](new_put_a_16_48000_cpu.html),
    [lock](new_put_a_16_48000_lock.html),
    [rps](new_put_a_16_48000_wrk.txt)
* * [50000](new_put_a_16_50000_wrk.txt)
* 32
* * [42000](new_put_a_32_42000_wrk.txt)
* * 43000: [alloc](new_put_a_32_43000_alloc.html),
    [cpu](new_put_a_32_43000_cpu.html),
    [lock](new_put_a_32_43000_lock.html),
    [rps](new_put_a_32_43000_wrk.txt),
* * [45000](new_put_a_32_45000_wrk.txt)
* * [50000](new_put_a_32_50000_wrk.txt)

Чем больше потоков, тем сильнее они мешают друг-другу, что приводит к значительному снижению производительности

Так мы добрались до 8 максимальных (т.е. по числу ядер).
* [alloc](new_put_a_2_8_48000_alloc.html)
* [cpu](new_put_a_2_8_48000_cpu.html)
* [lock](new_put_a_2_8_48000_lock.html)
* [rps](new_put_a_2_8_48000_wrk.txt)

5 min: [alloc](new_put_a_2_8_stable_long_alloc.html),
[cpu](new_put_a_2_8_stable_long_cpu.html),
[lock](new_put_a_2_8_stable_long_lock.html),
[rps](new_put_a_2_8_stable_long_wrk.txt)

Основные проблемы и заполнение очереди происходили при flush'ах таблицы, что вполне ожидаемо

#### GET

Здесь обратная ситуация, наши ответы достаточно долгие, а запросы наоборот быстрые, т.к. никаких данных считывать нам не нужно

И чем больше poolSize, тем больше допустимый RPS

### LinkedBlockingQueue

Несложно заметить, что у нас большие проблемы с локами. Связано это с тем, что в `ArrayBlockingQueue`
одна блокировка на чтение и запись, поэтому окончательно разделить селекторы от воркеров не получилось

Воспользуемся `LinkedBlockingQueue`, у которой уже 2 разные блокировки на чтение и запись

#### PUT

Как мы видим, несмотря на то, что время самих блокировок для селекторов значительно уменьшилось

Что повысило скорость поступления новых задач (поэтому был повышел queueCapacity до 200, т.к. он набирался слишком быстро).

Однако, сама очередь достаточно медлена. Из-за чего общая производительность страдает.

* [40000](new_put_l_32_40000_wrk.txt)
* 45000: [alloc](new_put_l_32_45000_alloc.html),
  [cpu](new_put_l_32_45000_cpu.html),
  [lock](new_put_l_32_45000_lock.html),
  [rps](new_put_l_32_45000_wrk.txt)
* [48000](new_put_l_32_48000_wrk.txt)
* [60000](new_put_l_32_60000_wrk.txt)

#### GET

На удивление, **GET'у** стало чуть лучше, но мне кажется, все в рамках погрешности

[rps](new_get_l_32_60000_wrk.txt),
[alloc](new_get_l_32_60000_alloc.html),
[cpu](new_get_l_32_60000_cpu.html),
[lock](new_get_l_32_60000_lock.html)

### Давайте попробуем поделить

Поделим селекторы и воркеров на группы, чтобы уменьшить блокировку

Как мы видим, мы смогли замедлить downgrade касательно времени работы

Однако, как-то значительно улучшить производительность это нам не помогло

#### PUT


* 40000:
  [alloc.html](1709758164_alloc.html)
  [cpu.html](1709758164_cpu.html)
  [lock.html](1709758164_lock.html)
  [wrk.txt](1709758164_wrk.txt)
* 50000:
    [alloc](1709758118_alloc.html),
    [cpu](1709758118_cpu.html),
    [lock](1709758118_lock.html),
    [rps](1709758118_wrk.txt)

  * 60000: [alloc](1709758089_alloc.html),
  [cpu](1709758089_cpu.html),
  [lock](1709758089_lock.html),
  [rps](1709758089_wrk.txt)

Локи в селекторах теперь незначительно, но тольку то ;)

## Вывод

Несмотря, на очевидность преимуществ, которые нам может подарить разделение селекторов

Оказывается, что на самом деле это довольно тонкая тема, в связи с многопоточностью и блокировками,
которые нужно не забывать учитывать

К сожалению, не успел посмотреть `DiscardPolicy` с обрывом сокета для первой в очереди. 
Это вполне могло повлиять на среднее время ответа