### Значения переменных
* Размер очереди `500`
* Размер кластера `3` - начальный
* flushThreshold `1 Мб`
* потоков в каждом workerPool `8`

Прогрев ноды, запускаем профилирование put запросов, потом сразу же запускаем профилирование get. 
Профилировщик запустим в трёх режимах: cpu, alloc и lock

* 128 соединений
* 16 потоков
* 1 минута

## Тест 1

Далее пробуем нагружать сервер предельными значениями Rps из прошлого этапа.
Проведя эксперименты оказалось, что сервис не справляется со старыми Rps и предельной нагрузкой(точкой разладки) для него является:
* `8000` запросов для put (было `23000`)
* `9000` запросов для get (было `39000`)

[wrk-put-8000rps.txt](..%2Fresults%2Fwrk-put-8000rps.txt), [wrk-get-9000rps.txt](..%2Fresults%2Fwrk-get-9000rps.txt)

На профилировании видим, что появились значительные изменения:
- около 40% cpu samples занимает работа httpClient, в том числе парки потоков, раньше не было
- около 5% занимают парки потоков, раньше не было. Скорее всего это связано с тем, что потоки воркеры блокируются на ожидании результата от потоков HttpClient. Хотелось бы чтобы во время ожидания ответа от других нод воркеры продолжали заниматься полезными делами.
Остальные параметры по cpu остались примерно в том же процентном соотношении как и в предыщем этапе.
- по профилированию блокировок, видно что их стало больше, все новые блокировки в HttpClient - работа запросов к другим нодам (они теперь составляют 18%)
- по профилированию аллокаций точно такая же итория как и с блокировками - около 65% аллокация приходится на HttpClient, раньше этих потоков не было и соответственноне было этих аллокаций
- cpu: на обработку dao теперь приходится 20% samples на get запросах и 17% samples на put (было 22% и 11% соответсвенно)

## Тест 2

Изменим размер кластера на `5`

Проведем такие же замеры.

[wrk-put-4500rps.txt](..%2Fresults%2Fcluster_size_5%2Fwrk-put-4500rps.txt), [wrk-get-5500rps.txt](..%2Fresults%2Fcluster_size_5%2Fwrk-get-5500rps.txt)

Получили ухудшение работы системы в сравнении с 3 нодами:
* `4500` запросов для put (было `8000`)
* `5500` запросов для get (было `9000`)
<br/>
Задержка также немного увеличилась 
* `avg - 57.03ms,  stdev - 62.71ms` для get запросов (была `avg - 3.65ms, stdev - 9.90ms`)
* `avg - 14.85ms, stdev - 38.11ms` для put запросов (былп `avg - 2.63ms, stdev - 6.49ms`)

## Итог

Получили, что реализация шардирования **НЕ** позволила сервису справиться с большей нагрузкой. Что связано с синхронной работой между шарами, поток ожидает ответ от другой ноды и блокируется, но еще больше доставляет проблемы то, что есл нно сделать более одного запроса к какой-то ноде, вынуждены ждать.
Увеличение кластера с 3 до 5 увеличило задержки для всех запросов, что скорее всего связано с работой распределённой системы в пределах одной машины.

## Возможные оптимизации
* Разделить запросы к текщей ноде и запросы к дрим нодам в разные очереди, так как запросы к текущей ноде можно быстро обработать.
* Каким-то образом расширить каналы связи между нодами, чтобы можно было делать больше одного синхронного запроса.