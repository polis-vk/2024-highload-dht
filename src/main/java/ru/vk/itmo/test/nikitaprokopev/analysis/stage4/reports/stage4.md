## Значения переменных
* Размер очереди `500`
* Размер кластера `3`
* flushThreshold `1 Мб`
* потоков в каждом workerPool `8`

Прогрев ноды, запускаем нагрузку put запросов, потом сразу же запускаем нагрузку get.

## Тест 1 - кворум 2/3
Далее пробуем нагружать сервер предельными значениями Rps из прошлого этапа.
Проведя эксперименты оказалось, что сервис не справляется со старыми Rps и предельной нагрузкой(точкой разладки) для него является:
* `3500` запросов для put (было `8000`) - 8 ms и отклонение 20 ms
* `3500` запросов для get (было `9000`) - 11 ms и отклонение 22 ms

Понижение производительности связано с тем, что при увеличении количества нод в кластере, увеличивается количество синхронных запросов между нодами, что приводит к блокировкам потоков.
Предполагается, что если установить ack в 1, то производительность увеличится, так как запросы будут ждать только одного успеха и не будут блокировать потоки дальше.

## Тест 2 - ack 1, from 3
Изменим ack на 1, from на 3 в скриптах wrk и проведем такие же замеры.
* `3500` запросов для put (было `8000`) - 14 ms и отклонение 25 ms
* `3500` запросов для get (было `9000`) - 127 ms и отклонение 158 ms
Производительность действительно увеличилась относительно кворума(2/3), но оказалась немного меньше прошлого этапа.
Попробуем выставить ack 3, from 3, предполагается что это будет наименнее производительно, так как запросы будут ждать всех нод.

## Тест 3 - ack 3, from 3
* `3500` запросов для put (было `8000`) - 12 ms и отклонение 27 ms
* `3500` запросов для get (было `9000`) - 8 ms и отклонение 14 ms
Производительность упала относительно ack 1, from 3 и оказалась меньше всех, что подтверждает предположение. Так как нам нужно дождаться ответа от всех нод, что занимает больше времени.

Тестирование с увеличением числа нод приводить не стал, так как проводил это сравнение и в прошлом этапе и производительность сильно уменьшилась, так как мы работаем в рамках одной машины.

## Профилирование

### cpu при 3/3

Сильных отличий между новой версией и stage 3 в контексте cpu-профилирования put запросов практически нет. Единственно, в новой реализации
пул воркеров теперь не только обрабатывает запрос или проксирует, но еще и занимается отправкой запросов другим нодам и получением ответов от них.
На это уходит 47% на get и 19% на put cpu samples, что вполне ожидаемо так как на гет запросах мы долго ждем пока ноды проведт поиск по ключу. В остальном все примерно так же как и в прошлом этапе.
Также немного увеличилось % samples на обработку списка нод в порядке приоритета после хэширования, с 0.1% до 0.2%.

### alloc при 3/3

Сильных отличий между новой версией и stage 3 также немного - увеличилось количество аллокаций в HttpClient, что вполне ожидаемо, так как теперь пул воркеров занимается отправкой запросов другим нодам и получением ответов от них. На это уходит 47% аллокаций, что вполне ожидаемо.
В остальном все примерно так же как и в прошлом этапе.
Увеличились незначительно аллокации в обработке списка нод в порядке приоритета после хэширования, с 1% до 2%.

### lock при 3/3

Теперь большинство локов происходит также в HttpClient, что вполне ожидаемо, так как теперь пул воркеров занимается отправкой запросов другим нодам и получением ответов от них. На это уходит ~98% локов, что вполне ожидаемо, запросов между нод стало много больше.

## Возможные оптимизации

* Как видно из результатов профилирования, большая часть времени работы CPU уходит на проксирование запросов по http
на другой узел и обратно, также при этом происходят более 90% локов и больше половины аллокаций.
Таким образом, как и в предыдущем stage, в первую очередь следует оптимизировать передачу данных между узлами кластера
заменив http протокол на другой, более эффективный, например, RPC, только в случае с репликацией такая оптимизация будет
еще полезней, так как число запросов в другие узлы вырастает.
* Для оптимизации возможно стоит рассмотреть поиск ключа на диске: подумать о фоновом компакшене, а также о других способах, которые позволили бы меньше "ходить" по диску.  
* Для ускорения работы пула воркеров можно поискать более оптимальную структуру данных для очереди задач воркеров, которая бы затрачивала меньше времени на блокировки, что особенно актуально для get-запросов, в которых появление очереди задач не дало особого выигрыша в производительности из-за появившихся блокировок взятия/размещения в очереди задач.


## Итог

Репликация снижает производительность системы, но придает системе отказоустойчивости,  так как при
неполадках с 1 или  несколькими узлами мы все равно сможет получить свои данные, если фактор репликации
позволяет и не упало больше from - ack + 1 узлов. Выбор оптимальных значений ack и from зависит от требований к
отказоустойчивости к сервису, важности данных, надежности аппаратной конфигурации кластера. В общем случае лучше всего
показал себя случай с from=3 и ack=2, который позволит в случае неполадок с одним из узлов (выход из строя двух узлов
короткий промежуток времени, да и еще именно тех, где лежит определенный ключ маловероятен) восстановить данные и
при котором показатели по времени и максимальному rate лучше, чем при более высоких значениях from или ack,
но все же хуже, чем при отсутствии репликации примерно в 3 раза. 