## Put 30K RPS, 8 каналов

Первым делом сравним версию без пула и с ним.

Начнем с 30K запросов в 8 соединений, поскольку в one-nio по дефолту количество Selector Thread'ов равно количеству
ядер, в моем случае это число равно 8.

Также 30К запросов было объемом запросов, который мог выдержать наш сервер без пула.

### Перцентили в версии с пулом:

```
 50.000%    0.93ms
 75.000%    1.31ms
 90.000%    1.69ms
 99.000%    6.29ms
 99.900%   11.21ms
 99.990%   17.97ms
 99.999%   21.44ms
100.000%   22.02ms
```

### Перцентили в версии без пула:

```
50.000%  628.00us
75.000%    0.93ms
90.000%    1.11ms
99.000%    8.65ms
99.900%   35.13ms
99.990%   62.40ms
99.999%   65.09ms
100.000%   65.38ms
```

Видим разницу в задержке.
В версии с пулом нашему Selector потоку теперь не нужно тратить свои ресурсы на выполнение логики запроса,
вместо этого он теперь может сразу заняться обработкой следующего запроса.

### CPU Flame Graph with Threads

![profile-cpu-many-threads-put-30000-1t-8c.png](aprof%2Fput%2Fprofile-put-30000-1t-8c%2Fprofile-cpu-many-threads-put-30000-1t-8c.png)
Поскольку диаграмма с отображением работы каждого треда нечитабельна, было решено отобразить график Worker и Selector
потоков отдельно.
При этом уже можно заметить, что количество Selector потоков, как я и говорил раньше, будет равно количеству ядер (8).

### Selector thread CPU Flame Graph

![selector-cpu-thread-put.png](aprof%2Fput%2Fprofile-put-30000-1t-8c%2Fselector-cpu-thread-put.png)

На графике с применением пула можно заметить, что Selector потоки теперь занимаются поллингом, чтением
и отправкой входящих запросов в пул воркерам (сюда же можно добавить взятие и освобождение блокировки на работу с
пулом).

### Worker thread CPU Flame Graph

![worker-cpu-thread-put.png](aprof%2Fput%2Fprofile-put-30000-1t-8c%2Fworker-cpu-thread-put.png)

Сами же воркеры занимаются чтением из очереди и ожиданием задач (это занимает львиную долю их работы, полагаю, что это
из-за недостаточного количества самих задач - воркеры попросту стоят без дела), выполнением логики работы с запросом,
также часть времени уходит на получение блокировки для работы с очередью.

В остальное время воркеры занимаются вставкой данных в бд, а также записью ответов в сокет (раньше этим занимались
Selector треды, тратя ~20% общего времени)

Для сравнения разницы по времени работы операций с версией без пула взглянем на CPU Graph без разделения на потоки.

### CPU Flame Graph

![profile-cpu-put-30000-1t-8c.png](aprof%2Fput%2Fprofile-put-30000-1t-8c%2Fprofile-cpu-put-30000-1t-8c.png)

| Работа селектора          | C пулом | Без пула | Объяснение (если нужно)                                                                                                                    |
|---------------------------|---------|----------|--------------------------------------------------------------------------------------------------------------------------------------------|
| Время работы селекторов   | 56%     | 100%     | -                                                                                                                                          |
| Поллинг запросов          | 40%     | 27%      | Теперь селектору в версии с пулом можно больше времени тратить на поллинг и скедулинг запросов, т.к. воркер треды забрали часть его работы |
| Обработка запроса         | 15%     | 56%      | В версии с пулом нам нужно лишь распарсить запрос и поставить его в очередь, не выполняя логику обработки запроса                          |
| Выполнение самого запроса | 1.5%    | 7%       | 1.5% в версии с пулом занимает постановка в очередь, когда в версии без пула селектор сам обрабатывает запрос                              |

В версии без пула действиями из таблицы ниже занимается Selector поток

| Работа воркера        | C пулом | Без пула | Объяснение (если нужно)                                                                                                           |
|-----------------------|---------|----------|-----------------------------------------------------------------------------------------------------------------------------------|
| Время работы воркеров | 40%     | -        | -                                                                                                                                 |
| Ожидание задачи       | 25%     | -        | Мы испытываем версию с пулом не загрузив ее по максимуму<br/>Поэтому четверть от общего времени занимает ожидание задач воркерами |
| Обработка запроса     | 12%     | 56%      | Без пула обработка запроса полностью приходится на селектор                                                                       |
| Вставить данные в бд  | 1.65%   | 7%       | Для версии без пула, этот ряд совпадает с рядом "Выполнение самого запроса" из таблицы выше                                       |
| Отправка ответа       | 9.37%   | 19.53%   | В версии с пулом, отправкой ответа занимается воркер тред, а не селектор                                                          |

Можно заметить, что при добавлении пула мы очень сильно разгружаем наши селектор потоки, однако платим за это временем
на работу с самим пулом, хоть оно и незначительно

### ALLOC Flame Graph

| Операция                            | C пулом | Без пула | Объяснение (если нужно)                                                                                                                         |
|-------------------------------------|---------|----------|-------------------------------------------------------------------------------------------------------------------------------------------------|
| Запуск воркеров                     | 52%     | -        | -                                                                                                                                               |
| Обработка нового запроса селектором | 40%     | 87%      | Версия с пулом занимается только парсингом запроса, <br/>тогда как версия без пула занимается еще и выполнением логики обработки самого запроса |
| Вставка в бд                        | 29%     | 26%      | -                                                                                                                                               |
| Формирование ответа                 | 11%     | 10%      | -                                                                                                                                               |
| Взять задачу из очереди             | 3%      | -        | -                                                                                                                                               |
| Запуск селекторов                   | 47.7%   | 94%      | В версии с пулом половина общей памяти уходит также на создание воркеров                                                                        |
| Select нового запроса               | 7%      | 6.5%     | -                                                                                                                                               |

В некоторых случаях соотношение по выделению изменилось в ~2 раза, т.к. добавились затраты на накладные расходные для
работы с воркерами и их создание

## Get 20K RPS, 8 каналов

Теперь рассмотрим как изменилась ситуация с Get запросами

В версии без пула наш сервер мог обрабатывать в среднем 20К RPS, поэтому сейчас рассмотрим аналогичную нагрузку

### Перцентили в версии с пулом:

```
 50.000%    1.16ms
 75.000%    1.64ms
 90.000%    2.15ms
 99.000%    3.21ms
 99.900%    7.52ms
 99.990%   22.66ms
 99.999%   25.92ms
100.000%   26.38ms
```

### Перцентили в версии без пула:

```
 50.000%  616.00us
 75.000%    0.91ms
 90.000%    1.08ms
 99.000%    2.07ms
 99.900%   25.47ms
 99.990%   44.26ms
 99.999%   45.60ms
100.000%   45.73ms
```

Также как и Put запросами видим разницу в задержке.
Причина аналогична - в версии с пулом мы разгрузили Selector поток

### Selector thread CPU Flame Graph

![selector-cpu-thread-get.png](aprof%2Fget%2Fprofile-get-20000-1t-8c%2Fselector-cpu-thread-get.png)

На графике с применением пула можно увидеть ровно ту же самую картину, что и при рассмотрении работы Put метода.
Селектор больше не занимается обработкой запроса самостоятельно

### Worker thread CPU Flame Graph

![worker-cpu-thread-get.png](aprof%2Fget%2Fprofile-get-20000-1t-8c%2Fworker-cpu-thread-get.png)

Аналогичная ситуация и с воркер тредами

## PUT 70K RPS, 1024 channel:

Как и в предыдущем этапе, определим точку разладки, начнем с 70К RPS в 1024 каналов, т.е. теперь 1024 запросов могут
одновременно попасть к нам на сервер

### Перцентили:

```
50.000%    1.01ms
75.000%    1.35ms
90.000%    1.64ms
99.000%    2.28ms
99.900%    9.30ms
99.990%   16.69ms
99.999%   21.17ms
100.000%   24.56ms
```

Перцентили показывают довольно радостную картинку, которую объясняют следующие строки:

```
Socket errors: connect 781, read 0, write 0, timeout 46079
Requests/sec:  16601.94
```

Т.е. мы видим, что настоящая нагрузка составляла всего 16К RPS и наш сервер смог одновременно обработать
только `1024-781=243` соединений

Я связываю это с тем, что 8 селекторов просто не успевают опрашивать такое количество соединений, учитывая тот факт, что
работа с соединениями не кратковременная, т.к. пишем мы в них постоянно.

### CPU Flame Graph

![profile-cpu-put-70000-8t-1024c.png](aprof%2Fput%2Fprofile-put-70000-8t-1024c%2Fprofile-cpu-put-70000-8t-1024c.png)

### Alloc Flame Graph

![profile-alloc-put-70000-8t-1024c.png](aprof%2Fput%2Fprofile-put-70000-8t-1024c%2Fprofile-alloc-put-70000-8t-1024c.png)

Профили также не показывает особых аномалий, т.е. сервер просто не принимал больше соединений

## PUT 70K RPS, 256 channel:

При уменьшении до 256 одновременных соединений можно увидеть, что количество ошибок также снизилось, но недостаточно

```
  7968590 requests in 2.00m, 509.16MB read
  Socket errors: connect 13, read 0, write 0, timeout 767
```

## PUT 70K RPS, 200 channel:

При уменьшении до 200 одновременных соединений ошибки пропали

### Перцентили:

```
50.000%    1.28ms
75.000%    1.75ms
90.000%    2.27ms
99.000%    3.45ms
99.900%   23.44ms
99.990%   61.53ms
99.999%   74.24ms
100.000%   81.15ms
```

На 99.9+ перцентилях можно заметить возрастание задержки, связываю это с тем, что чаще срабатывала сборка мусора и, в
целом, это больше похоже на артефакты, тк профили показывают аналогичную с предыдущими запусками картину

### CPU Flame Graph

![profile-cpu-put-70000-8t-200c.png](aprof%2Fput%2Fprofile-put-70000-8t-200c%2Fprofile-cpu-put-70000-8t-200c.png)

Больше времени начало уходить на ожидание работы с очередью при взятии и добавлении задачи, в остальном, графики
аналогичны

### Alloc Flame Graph

![profile-alloc-put-70000-8t-200c.png](aprof%2Fput%2Fprofile-put-70000-8t-200c%2Fprofile-alloc-put-70000-8t-200c.png)

Профиль аллокаций также особо не поменялся

### Lock Flame Graph

![profile-lock-put-70000-8t-200c.png](aprof%2Fput%2Fprofile-put-70000-8t-200c%2Fprofile-lock-put-70000-8t-200c.png)

По профилю блокировок можно увидеть, что основное время ходит на взятие блокировки при взятии задачи из пула и ее
добавлении туда.

Лишь 2.5% уходит на то, чтобы взять блокировку для записи в сессию

## GET 60K RPS, 200 channel:

### Перцентили:

```
 50.000%    1.21ms
 75.000%    1.64ms
 90.000%    2.08ms
 99.000%    2.96ms
 99.900%    5.02ms
 99.990%   18.14ms
 99.999%   35.87ms
100.000%   44.03ms
```

Можно увидеть, что при добавлении пула воркеров мы смогли в разы увеличить объем нагрузки, который может обрабатывать
наш сервер

## Улучшения

В качестве возможных улучшений мы можем поиграть со следующими параметрами:

* Размер пула - лучше выставить сразу в количество ядер начальное и максимальное значение, чтобы после появления
  нагрузки не тратить ресурсы на выделение новых потоков.
  Кроме этого, также можно выставить это значение выше количества ядер в нашей системе, если жесткий диск, с которым
  идет работа, поддерживает NVME.
  Поскольку сам NVME поддерживает параллельность потоков.
* Максимальный размер очереди - слишком большая очередь будет позволять храниться старым запросам, которые мы, возможно,
  уже не ждем.
  Слишком короткая может не справиться с большим количеством одновременных запросов.
* Максимальное время обработки запроса - зависит от SLA предъявляемых вашему сервису. В своей реализации я добавил
  игнорирование запросов, у которых истек таймаут (сейчас стоит 200 мс). На такие запросы клиенту отправляется ответ "408 Request Timeout".

Также можно поменять тип очереди, который мы используем в пуле:

* LinkedBlockingDeque - LIFO, интересный вариант, т.к. позволяет нам акцентировать внимание сервера на новых запросах
* ArrayBlockingQueue - FIFO, я считаю этот вариант предпочтительнее, т.к. при нем не возникает ситуации, когда старые
  запросы улетают в самый конец обработки очереди при большой нагрузке, как это делается в предыдущем варианте
* LinkedBlockingQueue - аналогичен предыдущему варианту, однако использует два lock'a - на взятие и добавление задачи в
  очередь, из-за чего время на ожидание блокировки снижается

Проверим насколько LinkedBlockingQueue работает быстрее ArrayBlockingQueue

### Перцентили с LinkedBlockingQueue:

```
 50.000%    1.32ms
 75.000%    1.78ms
 90.000%    2.30ms
 99.000%    3.36ms
 99.900%    8.88ms
 99.990%   27.26ms
 99.999%   32.27ms
100.000%   35.90ms
```

### Перцентили с ArrayBlockingQueue:

```
50.000%    1.28ms
75.000%    1.75ms
90.000%    2.27ms
99.000%    3.45ms
99.900%   23.44ms
99.990%   61.53ms
99.999%   74.24ms
100.000%   81.15ms
```

Можно увидеть, что задержка в самых редких ситуациях стала вдвое меньше

### Lock Flame Graph для LinkedBlockingQueue

![profile-lock-put-70000-8t-200c-LinkedBlockingQueue.png](aprof%2Fput%2Fprofile-put-70000-8t-200c-LinkedBlockingQueue%2Fprofile-lock-put-70000-8t-200c-LinkedBlockingQueue.png)

| Название операции          | ArrayBlockingQueue | LinkedBlockingQueue |
|----------------------------|--------------------|---------------------|
| Поставить задачу в очередь | 25.19%             | 42.91%              |
| Взять задачи из очереди    | 61.47%             | 54.76%              |
| Записать в сессию          | 13.33%             | 2.31%               |

При использовании LinkedBlockingQueue можно увидеть, что общее время на запись в сессию увеличилось, 
т.е. время на работу с блокировками для взятия и добавления задач в очередь уменьшилось. Связываю это как раз с использованием отдельных lock'ов в LinkedBlockingQueue 

Но, конечно, выбирать стоит `PriorityBlockingQueue` вкупе с ошибкой 'Payment required' для самых медленных запросов (шутка с лекции)

Также можно выбрать политику обработки запросов при заполнении очереди:

* AbortPolicy кидает исключение, которое мы можем обработать, чтобы оповестить наших клиентов о перегрузке сервера -
  вариант, который выбран сейчас
* DiscardPolicies определяет какие запросы мы будем выкидывать из очереди - новые или старые.
  В нашем случае это неподходящая политика, т.к. при ней клиент не узнает о том почему его запрос не выполнился и при
  мониторинге сервера мы также этого не увидим
* CallerRunsPolicy позволяет потоку, который добавил задачу в очередь самостоятельно распорядиться тем как обработать
  ситуацию при переполнении очереди. Нам данная политика не подходит по причине того, что мы не хотим нагружать наш
  Selector поток дополнительными задачами.

Также стоит отметить тот факт, что при текущих lua скриптах все потоки генерируют одинаковые данные, из-за этого
на диск записывается меньший объем данных, поскольку часть из них сжимается в памяти из-за большого числа дублей.