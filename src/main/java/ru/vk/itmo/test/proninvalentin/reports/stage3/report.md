Для данного этапа был реализован алгоритм Consistent Hashing для выбора узла, на который будут проксироваться запросы пользователя

Ради интереса, также был реализован Rendezvous Hashing, чтобы сравнить результаты распределения данных между узлами

### Сравнение алгоритмов шардирования

Ниже приведен список размеров файлов на каждой ноде

Каждый запуск добавлялось ~7 млн записей по 4 байта

### Consistent 3 Virtual Nodes
```
 0B	server_profiling_data8080
16M	server_profiling_data9080
14M	server_profiling_data10080
```

### Consistent hashing 50 Virtual Nodes
```
8,2M server_profiling_data8080
 11M server_profiling_data9080
8,2M server_profiling_data10080
```

Логично, что распределение стало лучше при добавлении виртуальных узлов, 
т.к. теперь диапазоны обрабатываемых ключей более равномерно "размазались" между виртуальными узлами

### Rendezvous hashing когда берем hash от конкатенации строк (nodeUrl + entityId)
```
11M server_profiling_data8080
11M server_profiling_data9080
11M server_profiling_data10080
```

### Rendezvous hashing когда берем hash от суммы хешей (nodeUrlHash + entityIdHash) (вариант с лекции)
```
30M	server_profiling_data8080
 0B	server_profiling_data9080
 0B	server_profiling_data10080
```

После лекции пришло осознание почему наш алгоритм на лекции выдавал такой плохой результат:

Дело в том, что при подходе с суммой хешей мы, по большей части, зависим от того, 
у какой ноды изначально получился самый большой хеш и при этом этот хеш должен быть достаточно далеко расположен от Integer.MAX_VALUE, 
чтобы при сложении с хешем `entityId` не словить переполнение (т.к. при переполнении мы с максимального значения уходим в минимальное).

В нашем случае это узел с портом "8080", т.к. на него пришлись практически все записи (записи на остальные узлы могли остаться в памяти)

Видим, что при 50 виртуальных нодах у Consistent Hashing результат аналогичен Rendezvous hashing (когда хешируем конкантенацию строк).

Разница по CPU между выбранными методами минимальная, я бы даже сказал, что на нее не стоит обращать внимания, если опираемся на перфоманс при выборе алгоритма

## CPU Hashing
### Consistent Hashing:
![consistentGetNodeByKey.png](aprof%2Fprofile-put-24000-8t%2FconsistentGetNodeByKey.png)

37 Сэмплов (0.09% от общего числа)

### Rendezvous Hashing:
![rendezvousGetNodeByKey.png](aprof%2Fprofile-put-26000-8t-rendezvous%2FrendezvousGetNodeByKey.png)

22 Сэмпла (0.05% от общего числа)

Также при Rendezvous Hashing нам дополнительно нужно аллоцировать память при сложении строк

## ALLOC Hashing
### Consistent Hashing:
![consistentGetNodeByKey.png](aprof%2Fprofile-put-24000-8t%2FconsistentGetNodeByKey.png)

199 Сэмплов (0.51% от общего числа)

### Rendezvous Hashing:
![rendezvousGetNodeByKey.png](aprof%2Fprofile-put-26000-8t-rendezvous%2FrendezvousGetNodeByKey.png)

580 Сэмпла (1.45% от общего числа)

Разница конечно видна, но опять же, кажется, что лучше прибегать к таким оптимизациям, когда других проблем с производительностью в системе нету

Посмотрим как изменилась нагрузка, которую выдерживает сервер после добавления шардирования
### Перцентили с PUT 24K RPS:

```
Перцентиль  было      стало
50.000%     1.14ms ->  1.28ms
75.000%     1.62ms ->  1.75ms
90.000%     2.00ms ->  2.27ms
99.000%     3.99ms ->  3.45ms
99.900%    17.26ms -> 23.44ms
99.990%    29.01ms -> 61.53ms
99.999%    30.98ms -> 74.24ms
100.000%   31.81ms -> 81.15ms
```

Конечно, если бы узлы кластера находились физически на разных машинах, то и 90 - 99 перцентили тоже существенно бы выросли, т.к. пришлось бы ждать ответа по сети, а не как сейчас - в пределах одной машины.
Однако сейчас выросло только время на 99.9+ перцентилях, это можно объяснить тем, что у нас выросло количество ожиданий блокировок, т.к. теперь мы также вынуждены синхронно ждать ответа от проксируемой ноды.  

### Перцентили с GET 28K RPS:

Ранее сервер выдерживал 60К RPS 
```
Перцентиль  было      стало
 50.000%    1.21ms ->  1.13ms
 75.000%    1.64ms ->  1.60ms
 90.000%    2.08ms ->  1.96ms
 99.000%    2.96ms ->  4.29ms
 99.900%    5.02ms -> 45.82ms
 99.990%   18.14ms -> 53.41ms
 99.999%   35.87ms -> 55.71ms
100.000%   44.03ms -> 56.32ms
```

Аналогичная ситуация и здесь, однако существенно вырос 99.9 перцентиль, скорее всего, это также связано с добавлением ожидания блокировок при проксировании

## Анализ Flame Graph

## PUT запросы
### ALLOC

![profile-alloc-put-24000-8t.png](aprof%2Fprofile-put-24000-8t%2Fprofile-alloc-put-24000-8t.png)

Из нового добавилось:

* Внутри воркеров теперь идет работа с `HttpClient` (
15% сэмплов от общего количества занимает обработка проксируемого запроса: построение http request'a и его отправка. 
При этом обработка запроса на самом воркере занимает 4.7%.))
* Добавилась работа (5.56% сэмплов) `HttpClient.SelectorManager` для управления соединениями нашего клиента
* 37% работа с `responseAsync` внутри `HttpСlient`.
  * 25% работа на работу с `СompletableFuture`, видимо, при отправке запроса в `HttpСlient`.
* 20% работа с `SequentialScheduler` внутри `HttpСlient`.
* 5.77% работа с `SocketTube` для `HttpСlient`.

Видно, что очень много ресурсов уходит на синхронную работу с `HttpClient` - нужно перейти на асинхронную, чтобы разгрузить воркеров, 
т.к. сейчас они просто ждут ответа от другой ноды, не занимаясь ничем полезным. 
При этом, если нода зависнет, то зависнет и воркер, пока не упадет по таймауту.

Возможно, стоит поменять и саму реализацию http клиента или версию протокола, чтобы снизить накладные расходы для работы с ними.
В качестве кандидата можно рассмотреть GRPC.

### CPU

![profile-cpu-put-24000-8t.png](aprof%2Fprofile-put-24000-8t%2Fprofile-cpu-put-24000-8t.png)

Не увидел значимых отличий в новых операциях с профилем alloc запросов, единственное, что теперь видно как много простаивают воркеры
при ожидании задачи из очереди (21%) и на ожидании окончания отправки запроса к проксируемой ноде (6%) 

### Профиль lock, put запросы

![profile-lock-put-24000-8t.png](aprof%2Fprofile-put-24000-8t%2Fprofile-lock-put-24000-8t.png)

Здесь снова, в обоих случае, мы видим в основном блокировки на:
* `SelectorManager'а`, которым пользуется http клиент
* Также заметил, что в основном блокировки http клиента заканчиваются на методе `register` при работе с сокетами

В репозитории openjdk видно, что данный метод отвечает за регистрацию асинхронных эвентов, при этом используя lock, чтобы атомарно проверять не закрыт ли текущий SelectorManager и зарегистрировать в нем новый эвент 

![registerAsync.png](aprof%2Fprofile-put-24000-8t%2FregisterAsync.png)

## GET запросы

### ALLOC
![profile-alloc-get-28000-8t.png](aprof%2Fprofile-get-28000-8t%2Fprofile-alloc-get-28000-8t.png)
Не заметил, чтобы появились новые аллокаций, отличные от тех, что были при put запросах.
Единственное, что добавились аллокации при получении записей из бд с нашей или проксируемой ноды

### CPU
![profile-cpu-get-28000-8t.png](aprof%2Fprofile-get-28000-8t%2Fprofile-cpu-get-28000-8t.png)
Здесь тоже не увидел отличий, т.к., по сути, добавились только расходы на работу по http

### LOCK
![profile-lock-get-28000-8t.png](aprof%2Fprofile-get-28000-8t%2Fprofile-lock-get-28000-8t.png)

Добавились расходы на получение тела запроса в методе `jdk/internal/net/http/Http1Response$BodyReader.tryAsyncReceive`, т.к. раньше при проксировании запросов нам приходил ответ в виде 201 статус кода и пустого тела, а теперь мы получаем в теле ответа запись из бд.

При GET мы тратили 0.19% на получение тела

При PUT мы тратим 23% на получение тела

### Выводы

* В качестве алгоритма для определения ноды для обработки запроса можно использовать Rendezvous или Consistent hashing. 
Данные алгоритмы уменьшают процент данных, которые необходимо перераспределить между узлами кластера при изменении количества узлов
  * Consistent hashing обладает асимптотикой O(log(N)) (N - количество узлов в кластере), но при выходе узла из строя или его добавлении, 
  необходимо изменять состояние кольца с виртуальными узлами, чтобы переспределить нагрузку между новым составом узлов.
  При этом также можно изменить состояние кольца, когда мы видим, что некоторые узлы нагружены больше остальных, чтобы их разгрузить.
  * Rendezvous Hashing обладает асимптотикой O(N) (N - количество узлов в кластере), нету прямой возможности управлять нагрузкой узлов, зато реализация сильно проще. 

* Добавили шардирование - теперь можем горизонтально масштабироваться, однако стоимость обработки запросов выросла из-за того, что необходимо делать синхронные запросы в другие ноды кластера.
  В целом наибольший процент ресурсов уходит на работу с HttpClient, который можно было бы заменить на GRPC вкупе с асинхронным взаимодействием
  вместо синхронного ожидания.

### Предложения по возможным улучшениям

* Попробовать редиректить клиента напрямую на ноду в кластере с нужными данными, чтобы не тратить время нашего сервера на ожидание ответа и его отправку. 
Либо же добавить асинхронную обработку запросов, чтобы опять же не заставлять потоки синхронно ожидать ответа от другой ноды.

* Попробовать использовать пул соединений - по одному http client'у на взаимодействие с каждой нодой, чтобы не блокировать других воркеров пока текущий ждет ответа, однако асинхронный вариант работы с http client'ом выглядит лучше.

* Рассмотреть возможность добавления балансировщика перед узлами нашего кластера с алгоритмом определения шарда, чтобы он сам принимал решение на какую ноду отправить запрос.
Либо же перейти на клиентскую балансировку - пусть клиенты сами понимают на какой узел им пойти, особенно если в роли клиентов выступают такие же сервисы во внутренней сети.

* Добавить ретраи на неудачные запросы, чтобы снизить процент, получаемых клиентом, ошибок. 
Однако здесь нужно быть аккуратным, т.к. неправильно настроенная политика ретраев может только усугубить ситуацию с упавшими нодами.
В таком случае, можно рассмотреть техники: exponential backoff и jitter для определения времени между ретраев.

* Рассмотреть другие хэш функции, т.к. кроме равномерного распределения по шардам, нам, возможно, не важна криптографическая стойкость и мы можем ей пожертвовать во благо лучшей производительности.
Хотя здесь, конечно, также стоит понимать, что при простой хеш функции злоумышленнику будет проще подобрать ключи для того, чтобы целенаправленно положить нам определенный узел кластера