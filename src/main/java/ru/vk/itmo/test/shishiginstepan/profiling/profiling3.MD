В рамках сравнительного профилирования результатов второго и третьего этапа, я проведу повторные замеры для второй лабы,
учитывая советы по поиску и анализу точки разладки от Александра Пащенко.

**Переосмысление данных прошлого этапа**

Точки разладки будем рассматривать для двух сценариев нагрузки

1. Очень много рандомных вставок
2. Очень много рандомных чтений на большом количестве ss-tables (около 600)

Значения второго этапа (будем считать их точкой отсчета)

1. **запись**


| %     | l        |
|-------|----------|
| 90    | 17.17 ms |
| 99    | 65.66 ms |
| 99.99 | 90 ms    |
Requests/sec:  94905.13

данные на запись вопросов не вызвали, оставлю как есть

2. **чтение**

   Первым делом меня смутило то что запросы на чтение отрабатывают в разы быстрее, хотя по своей сути являются сложнее
   простой вставки.

90.000% 2.13ms
99.000% 6.54ms
99.999% 18.32ms

| %     | l       |
|-------|---------|
| 90    | 2.13ms  |
| 99    | 6.54ms  |
| 99.99 | 18.32ms |

Requests/sec:  99931.35

Сначала я предположил что я тестирую mem таблицу вместо доступа к данным на диске. Рестартанув сервис и не прогревая mem
таблицу я получил те же самые результаты.
Следующим я подумал на page cache.
Я очистид page кеш и провел замеры заново.

Замеры после очистки page cache:

| %     | l       |
|-------|---------|
| 90    | 1.67 s |
| 99    | 1.79 s |
| 99.99 | 1.8 s |

Requests/sec:  92007.16

Мои 100000 RPS с максимальными 18 ms задержки превратились в тыкву.

Итеративно очищая page cache я выяснил новую точку разладки - она находится на 10000 рпс ниже и от неё я решил
отталкиваться в сравнении


| %     | l        |
|-------|----------|
| 90    | 55.67 ms |
| 99    | 89.79 ms |
| 99.99 | 97.8 ms  |
Requests/sec:  91942.13

**Измерения третьего этапа**

Короткое резюме моего решения:

1. Использую consistent hashing для определения места ключа в системе (murmur3 в кач-ве hash функции)
2. Исползую клиент one nio
3. Решил немного запариться и написал класс который имплементирует интерфейс dao, так что в сервере почти ничего не
   поменялось, а вся работы по распределению по нодам и поиск на других нодах выпонляется в отдельном классе.

Подход к проведению замеров:

Каждая нода - отдельный java процесс на локальной машине со своим ограничением памяти в 128m.
После каждого замера выполняется перезапуск процессов с очищением page cache.

**Начнем с наполнения БД с помощью распределения данных одной нодой на другие.**

Примерно в точке 42-43 тыс. RPS находится точка разладки, это примерно 45% от RPS данного профиля нагрузки из второго
этапа.

| %     | l       |
|-------|---------|
| 90    | 2.02 ms |
| 99    | 4.45 ms |
| 99.99 | 16.1 ms |
Requests/sec:  41990.99

**Замеры чтения:**


| %     | l        |
|-------|----------|
| 90    | 2.69 ms  |
| 99    | 22.30 ms |
| 99.99 | 31.98 ms |
Requests/sec:  52988.70

Операции чтения потеряля меньше в процентом соотношении, RPS составил порядка 55% от тех же показателей на 1 ноде.

Я решил проверить соотношение количества данных на нодах:

31K db-data-0 <br>
31K db-data-1 <br>
33K db-data-2 <br>

Благодаря концепции v-node которую я реализовал получается очень близкое к равномерному распределение данных.

Я решил проэксперементировать и уменьшить количество v-nodes до 3 (сейчас стоит 256)

32K db-data-0<br>
60K db-data-1<br>
19K db-data-2<br>

несмотря на испольование murmur3 хеширования, маленькое количество V-nod не дает хорошего распределения ключей.
Тайминги причем остались те же (погрешность 5-10%) при записи через ноду 0, т.к. она записывала примерно тот-же объем к себе и остальное распределялось на другие ноды.

Чтение я решил проверить с двух нод:
с той в которой больше всего данных и в самой "обделенной" распределением ключей.

Нода с наибольшим владением ключами показала кратный выигрыш по задержке относительно почти "пустой" ноды(порядка 80% на одинаковых rps), что еще раз подтверждает важность равномерного распределения данных

