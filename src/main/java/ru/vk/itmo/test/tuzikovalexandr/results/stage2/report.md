## PUT 500 10s 8 threads

Сначала попробуем подать нагрузку, используя 1 поток и 1 соедниене.

```
 50.000%    2.49ms
 75.000%    3.52ms
 90.000%   16.51ms
 99.000%  147.07ms
 99.900%  191.87ms
 99.990%  194.94ms
 99.999%  194.94ms
100.000%  194.94ms
```

Далее увеличим количество потоков до 8 и количествое соединений до 64.
Результат оказался следующий: 

```
 50.000%    2.54ms
 75.000%    3.60ms
 90.000%   13.36ms
 99.000%   51.97ms
 99.900%   82.50ms
 99.990%   87.10ms
 99.999%   87.10ms
100.000%   87.10ms
```

По результатам можно отметить, что на 90-ом перцентиле время уменьшается, 
а на 99-ом и далее даже в 2 раза.

Стоит отметить, что при проведении экспериментов на второй стадии, у виртуальной машины было
изменено количество доступных процессоров с 2 до 4 и объем оперативной памяти с 4 ГБ до 8 ГБ.
Однако, ни к каким улучшениям это не привело.

## PUT 25000 30s 8 threads

Теперь проведем эксперимент на 25000 запросов, так как это было некоторой точкой, 
когда сервер начинал плохо работать.

```
 50.000%   17.17s 
 75.000%   21.45s 
 90.000%   24.82s 
 99.000%   27.72s 
 99.900%   27.84s 
 99.990%   27.87s 
 99.999%   27.92s 
100.000%   27.92s 
```

```
 50.000%   10.65ms
 75.000%  107.65ms
 90.000%  308.99ms
 99.000%  525.82ms
 99.900%  586.75ms
 99.990%  662.53ms
 99.999%  691.20ms
100.000%  695.81ms
```

Аналогично как и с предыдущим экспериментом можно ответить, что прирост в скорости значительный.

## Анализ Flame Graph (CPU PUT запросов)

Далее проведем эксперимент профилирования с несколькими соединениями и потоками, чтобы посмотреть, 
как теперь распределеяется трата ресурсов. Как входые параметры было подано 25000 запросов, 8 потоков и
64 соединения. На диаграмме put-profile-8c-cpu.html предствлены результаты эксперимента.

Можно отметить, что теперь селекторы не тратят свои ресурсы на выполнение логики запроса.
Так мы высвободили ресурсы для его базовой задачи - обработки следующих запросов (около 20%). 
Также это чтение и отправка входящих запросов и поллинг. Еще 1% тратится на работу с блокировкой у очереди.

Обработкой логики запросов и чтением их из очереди занимаются сами воркеры (> 70%). Тут также можно отметить,
что чуть более 15% уходит на работу с очередью. 

В остальном, как это и было раньше, но у селекторов, теперь воркеры занимаются работой с бд и записью ответов в сокет.

Как итог из общей тенденции: происходит выигрыш во времени работы селектора, обработки запроса им же, но теперь
добавляется нагрузка на поллинг.

## Анализ Flame Graph (PUT запросов) работа селекторов и потоков

Был проведен эксперимент, где на вход подавался 1 поток и 8 соединений. Напомню, что виртуальной машине было
выделено 4 ядра.

Результаты на диаграмме put-profile-1th-8c.html.

Очень интересное замечение, что количество селекторов в результате получилось всего 3.
Есть предположение, что это ограничение ОС или гипервизора. В случае виртуальных машин, гипервизоры, 
такие как VMware ESXi, Microsoft Hyper-V, или Oracle VirtualBox, могут вводить определённые 
ограничения на использование аппаратных ресурсов виртуализированными системами, что 
также может сказаться на количестве эффективно используемых потоков.

## Flame Graph (ALLOC PUT запросов)

Результаты на диаграмме put-profile-8th-alloc.html.

Все как и аналогично выше, так как теперь логику обработки запросов взяли на себя воркеры, нагрузка на селекторы
сильно уменьшилась. Но добавились незначительные расходы ресурсов на создание воркеров и их работу.

## GET 25000 30s 8 threads

При анализе улучшений GET-запросов сразу дадим высокую нагрузку. И, как аналогично, сначала будет 1 поток, 1 соединение,
потом 8 потоков, 64 соединения.

```
 50.000%   18.06s 
 75.000%   22.99s 
 90.000%   25.46s 
 99.000%   27.46s 
 99.900%   27.66s 
 99.990%   27.67s 
 99.999%   27.69s 
100.000%   27.69s 
```

```
 50.000%  312.58ms
 75.000%  954.88ms
 90.000%    1.43s 
 99.000%    1.85s 
 99.900%    1.96s 
 99.990%    2.01s 
 99.999%    2.03s 
100.000%    2.03s 
```

Как можно заметить результаты улучшились в 10 раз. И запросов при наличии большего числа потоков и соединений
обрабатывается также больше. Как и с PUT-запросами причины улучшения - разгрузка селекторов.

## Анализ Flame Graph (CPU GET запросов)

Далее проведем эксперимент профилирования с несколькими соединениями и потоками, чтобы посмотреть,
как теперь распределеяется трата ресурсов уже для GET-запросов. Как входые параметры было подано 25000 запросов, 
8 потоков и 64 соединения. На диаграмме get-profile-8c-cpu.html предствлены результаты эксперимента.

Аналогично, как и с PUT-запросами ситуация с воркерами.

## Новая точка разладки

Так как изначально максимальный размер очереди выставлен 1000, попробуем и количество соединений увеличить 
до такого числа. Входные параметры передадим 8 потоков, 1050 соединений и 80000 запросов. 

```
 50.000%   79.87ms
 75.000%  291.84ms
 90.000%  519.42ms
 99.000%    1.24s 
 99.900%    1.53s 
 99.990%    1.70s 
 99.999%    1.74s 
100.000%    1.76s 
```

Вроде, по результам довольно даже неплохо с учетом того ошибок соединения. Но на 99-ом перцентиле есть скачок.

```
Socket errors: connect 35, read 0, write 0, timeout 490
```

Кажется, что размер очереди в 1000 даже достаточно оптимален.

## Flame Graph (CPU, ALLOC PUT запросов)

Результаты в put-profile-8th-1050con-80000-cpu.html и put-profile-8th-1050con-80000-alloc.html. 
Вроде, никаких аномалий не выявлено.

## PUT 80000 30s 8th 1000con

Уменьшив количество соединений, получилось избавиться от ошибок. Результаты перцентилей:

```
 50.000%  523.78ms
 75.000%  954.37ms
 90.000%    1.33s 
 99.000%    1.75s 
 99.900%    2.04s 
 99.990%    2.13s 
 99.999%    2.17s 
100.000%    2.19s 
```

Профили эксперимента в файлах put-profile-8th-1000con-80000-cpu.html,
put-profile-8th-1000con-80000-alloc.html и put-profile-8th-1000con-80000-lock.html. Сравнивая профили
предыдущего эксперимента и этого особых изменений не выявлено. 
По профилю блокировок видно, что работа с ними не занимает много времени. То есть при взятии задачи и
ее добавлении расходуется 20% ресурсов. 

## Возможные улучшения

### Политика обработки запросов

1. CallerRunsPolicy. Сам поток будет распоряжаться задачей и как обрабатывать переполнение. Не подходит, так как
будет лишняя нагрузка на селекторы.
2. DiscardPolicies. Не совсем подходит, так как ошибок он не выдает и непонятно, почему запрос не был выполнен,
но определяет какие запросы будут убраны из очереди (более новые или более старые).
3. AbortPolicy. Кажется, что наиболее подходящий варинт, так как при отклонении обработки выдает исключение,
которое можно обработать и тем самым оповестить пользователя.

### Входные параметры

1. Как было видно из экспериментов размер очереди влияет на работу сервиса. Если очередь большая, то
может возникуть хранение слишком старых заспросов, а при слишком маленькой есть шанс, что сервис просто их
не сможет все обработать.
2. Количество потоков. Было довольно сложно понять их влияние. И даже до сих пор. Уже было отмечено в отчете,
что даже выделив 8 потоков (а виртуальной машине 4 ядра), были сознады всего 3, как я понял. Так что здесь 
все зависит только от физических возможностей железа.

### Очередь

1. ArrayBlockingQueue. Работает по принципу FIFO. Не концентрируем внимание на свежести запроса. Возможно,
более предпочтительный вариант, но в этой реализации выбран следующий.
2. LinkedBlockingDeque. Работает по принципу LIFO. Обрабатываем свежие запросы.
3. PriorityBlockingQueue. Просто поиздеваться надо пользователями)))

