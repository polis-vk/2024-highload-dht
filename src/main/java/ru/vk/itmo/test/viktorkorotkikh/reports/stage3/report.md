# Stage 3

- [Stage 3](#stage-3)
    * [Конфигурация](#Конфигурация)
    * [PUT](#put)
        + [CPU profile](#cpu-profile)
        + [Alloc profile](#alloc-profile)
        + [Lock profile](#lock-profile)
    * [GET](#get)
        + [CPU profile](#cpu-profile-1)
        + [Alloc profile](#alloc-profile-1)
        + [Lock profile](#lock-profile-1)

## Конфигурация

wrk2 - 64 connections, 4 threads

Конфигурация кластера - 3 ноды, запущенные в отдельных процессах. Профилируем ноду, на которую шлём все запросы.

## PUT

[PUT-60k.txt](PUT-60k.txt)

```
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency     1.38ms    1.42ms  38.14ms   97.36%
    Req/Sec    15.81k     1.22k   26.44k    74.46%
  Latency Distribution (HdrHistogram - Recorded Latency)
 50.000%    1.22ms
 75.000%    1.70ms
 90.000%    2.18ms
 99.000%    3.87ms
 99.900%   21.89ms
 99.990%   30.17ms
 99.999%   35.36ms
100.000%   38.17ms
```

![PUT-60k-histogram.png](PUT-60k-histogram.png)

### CPU profile

[PUT-60k-cpu.html](PUT-60k-cpu.html)

На профиле теперь видно, что очень много сэмплов занимает богатый внутренний мир httpClient.

Часть из них - отправка и приём запроса.

Из интересного - в самом методе sendClusterRequest не видно самой отправки запроса, так как я использую метод sendAsync, 
получаю CompletableFuture и вызываю у него метод timedGet. Отправка/получение ответа происходит в другом executor'е 
(который был передан при создании http клиента), но сам результат получаем в вызывающем.

Таким образом, теперь мы получаем, что основная работа при записи данных, уходит на взаимодействие по сети, так как 
локальная запись происходит очень быстро (2.67% всех сэмплов).

### Alloc profile

Аллокации тоже изменились (раньше было так [PUT-60k-4threads-alloc.png](../stage2/PUT-60k-4threads-alloc.png)):

[PUT-60k-alloc.html](PUT-60k-alloc.html)

Достаточное количество выделяется на создание заголовков запроса (23.85%).

Отправка тела запроса состоит в основном из аллокаций MinimalFuture, так как асинхронная отправка имеет очень много
стадий CompletableFuture и в очень многих местах создаются новые объекты.

При приёме ответа выделяется память на чтение заголовков - тут "по-честному" выделяем массивы байт, а не future. Также
выделяем массивы для чтения тела ответа. Но без выделения MinimalFuture тут тоже не обходится.

При создании запроса нам приходится каждый раз создавать URI, так как запросы у нас параметризованные (передаем id).
Также видим аллокации на заголовки при создании HttpRequestImpl.
Ещё одна аллокация заголовков происходит из-за того, что httpClient превращает наши заголовки в список ByteBuffers.


### Lock profile

[PUT-60k-lock.html](PUT-60k-lock.html)

Видим, что в основном блокировки происходят на `EPollSelectorImpl::wakeup` - он вызывается для пробуждения селектора, 
который в данный момент может находиться в ожидании готовности сокета. Это связано с асинхронной обработкой HTTP 
запросов и ответов.

Потоки регистрируют разные события AsyncEvent (запись в сокет, подписка на чтение из сокета, продолжить чтение из 
сокета и др.), которые пробуждают поток, который делает `socket::select`. Метод `EPollSelectorImpl::wakeup` 
синхронизирован, поэтому и видим локи. Частые асинхронные операции ввода-вывода приводят к такому количеству сэмплов.

Всё также видим локи на очереди. Только теперь это локи не только на основном нашем executor'е, но и на executor'е http 
клиента.

## GET

### CPU profile



### Alloc profile



### Lock profile


