# Stage 4

- [Stage 4](#stage-4)
    * [Конфигурация](#Конфигурация)
    * [PUT](#put)
        + [CPU profile](#cpu-profile)
        + [Alloc profile](#alloc-profile)
        + [Lock profile](#lock-profile)
    * [GET](#get)
        + [CPU profile](#cpu-profile-1)
        + [Alloc profile](#alloc-profile-1)
        + [Lock profile](#lock-profile-1)
  * [GET](#get-key-miss)
      + [CPU profile](#cpu-profile-2)
      + [Alloc profile](#alloc-profile-2)
      + [Lock profile](#lock-profile-2)

## Конфигурация

wrk2 - 64 connections, 4 threads

Конфигурация кластера - 3 ноды, запущенные в отдельных процессах. Профилируем ноду, на которую шлём все запросы.

Запросы без параметров ack и from -> по умолчанию реплицирование 2 из 3

## PUT

[PUT-60k.txt](PUT-60k.txt)

```
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency    17.29s     7.29s   30.15s    57.82%
    Req/Sec     7.43k   102.97     7.58k    60.00%
  Latency Distribution (HdrHistogram - Recorded Latency)
 50.000%   17.19s 
 75.000%   23.58s 
 90.000%   27.51s 
 99.000%   29.87s 
 99.900%   30.10s 
 99.990%   30.13s 
 99.999%   30.15s 
100.000%   30.16s 
----------------------------------------------------------
  1790139 requests in 1.00m, 114.38MB read
Requests/sec:  29835.72
Transfer/sec:      1.91MB
```

Сразу видно, что точка разладки уехала вниз - нужно уменьшать нагрузку, 60k RPS мы уже не выдерживаем. Максимум теперь 
30k RPS, это видно из отчёта wrk - больше пропихнуть он не смог.

Путём экспериментов была выбрана точка разладки в 25к RPS.

[PUT-25k.txt](PUT-25k.txt)

```
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency     1.76ms    1.18ms  16.37ms   89.30%
    Req/Sec     6.59k   562.16    10.00k    74.85%
  Latency Distribution (HdrHistogram - Recorded Latency)
 50.000%    1.54ms
 75.000%    2.08ms
 90.000%    2.68ms
 99.000%    7.21ms
 99.900%   11.73ms
 99.990%   14.84ms
 99.999%   15.73ms
100.000%   16.38ms
```

![PUT-25k-histogram.png](PUT-25k-histogram.png)

Очевидно, что точка разладки сместилась из-за того, что мы делаем обязательную пересылку по сети из-за фактора 
реплицирования. При этом в некоторых случаях у нас нет локальной записи на ноду, на которую мы отправили запрос, и есть 
2 последовательных запроса по сети к двум другим нодам.

### CPU profile

[PUT-25k-cpu.html](PUT-25k-cpu.html)

На профиле стало видно работу GC - 6.26% от общего числа сэмплов.

В остальном профиль в целом такой же, как и в [предыдущем stage](../stage3/PUT-60k-cpu.html).

Богатый внутренний мир HttpClient никуда не делся. Количество сэмплов на локальную запись уменьшилось до 1.76%, что 
логично так как мы теперь ещё больше ходим по сети.

### Alloc profile

Аллокации тоже сильно не изменились (раньше было так [PUT-60k-alloc.html](../stage3/PUT-60k-alloc.html)):

[PUT-25k-alloc.html](PUT-25k-alloc.html)

// TODO

Достаточное количество выделяется на создание заголовков запроса (23.85%).

Отправка тела запроса состоит в основном из аллокаций MinimalFuture, так как асинхронная отправка имеет очень много
стадий CompletableFuture и в очень многих местах создаются новые объекты.

При приёме ответа выделяется память на чтение заголовков - тут "по-честному" выделяем массивы байт, а не future. Также
выделяем массивы для чтения тела ответа. Но без выделения MinimalFuture тут тоже не обходится.

При создании запроса нам приходится каждый раз создавать URI, так как запросы у нас параметризованные (передаем id).
Также видим аллокации на заголовки при создании HttpRequestImpl.
Ещё одна аллокация заголовков происходит из-за того, что HttpClient превращает наши заголовки в список ByteBuffers.


### Lock profile

[PUT-25k-lock.html](PUT-25k-lock.html)

// TODO

Видим, что в основном блокировки происходят на `EPollSelectorImpl::wakeup` - он вызывается для пробуждения селектора, 
который в данный момент может находиться в ожидании готовности сокета. Это связано с асинхронной обработкой HTTP 
запросов и ответов.

Потоки регистрируют разные события AsyncEvent (запись в сокет, подписка на чтение из сокета, продолжить чтение из 
сокета и др.), которые пробуждают поток, который делает `socket::select`. Метод `EPollSelectorImpl::wakeup` 
синхронизирован, поэтому и видим локи. Частые асинхронные операции ввода-вывода приводят к такому количеству сэмплов.

Всё также видим локи на очереди. Только теперь это локи не только на основном нашем executor'е, но и на executor'е http 
клиента.

## GET

База объемом ~1.5G, каждая нода хранит около 517mb.

[GET-30k.txt](GET-30k.txt)

```
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency    23.30s     9.61s   40.01s    57.65%
    Req/Sec     2.50k     6.85     2.51k    75.00%
  Latency Distribution (HdrHistogram - Recorded Latency)
 50.000%   23.30s 
 75.000%   31.64s 
 90.000%   36.63s 
 99.000%   39.65s 
 99.900%   39.94s 
 99.990%   40.01s 
 99.999%   40.04s 
100.000%   40.04s 
----------------------------------------------------------
  599960 requests in 1.00m, 29.40MB read
  Non-2xx or 3xx responses: 17980
Requests/sec:   9999.36
Transfer/sec:    501.70KB
```

Показатели latency GET запросов тоже стали выше относительно [прошлого результата](../stage3/GET-30k.txt).
Да и явно точка разладки стала ниже.

Это связано с тем, что поиск по всем sstable осуществляется не на одной ноде, а на 2-х.

Максимум теперь 10k RPS, это видно из отчёта wrk - больше пропихнуть он не смог.

Путём экспериментов была выбрана точка разладки в 8к RPS.

![GET-8k-histogram.png](GET-8k-histogram.png)

### CPU profile

[GET-8k-cpu.html](GET-8k-cpu.html)

// TODO

Теперь у нас локальное чтение занимает ~34% сэмплов, а общение по сети с другими нодами около 44% (если объединить все 
сэмплы, связанные с работой HttpClient, кроме работы с очередью, так как основной пул потоков использует ту же очередь).

В целом количество сэплов локального чтение совпадает с распределением запросов в кластере - каждая нода должна 
обрабатывать 33.33% запросов к кластеру. Напрямую сравнивать количество сэмплов и количество запросов нельзя, так как 
ноды не только занимаются поиском значений по ключам, но и также имеют операции ввода-вывода и прочие расходы, но данные
значения определённо коррелируют.

### Alloc profile

[GET-8k-alloc.html](GET-8k-alloc.html)

// TODO

И снова богатый внутренний мир HttpClient приводит чуть ли не к 80% аллокаций от общего числа.
Достаточно большое количество аллокаций (26%) приходится на выделение памяти под заголовки запроса для последующей их 
отправки (`Http1Exchange::sendHeaderAsync`).

При всём этом аллокации `MemorySegment` при чтении занимают всего ~1% (база была перезагружена, так что никаких ключей 
в memTable лежать не могло).

Конечно такое положение дел заставляет ужаснуться, так как уж очень сильный эффект имеет HttpClient. Возможно, эти 
аллокации можно каким-то образом сократить, но это весьма нетривиальная задача, так как HttpClient имеет достаточно 
сложную архитектуру.

### Lock profile

[GET-8k-lock.html](GET-8k-lock.html)

// TODO

На локах хорошо видно, что на блокировках на очереди HttpClient больше сэмплов, чем на блокировках на очереди, которую 
использует основной пул потоков. Связано это может быть с тем, что при асинхронной обработке запросов HttpClient создаёт
различные CompletableFuture, которые отправляются в ExecutorService.
Также заметно, что блокировки в методе take превалируют над блокировками в методе offer - вероятно тут потоки ожидают 
ответа от ноды кластера.

Так же как и при PUT запросах, в основном блокировки происходят на методе `EPollSelectorImpl::wakeup`.

## Get key miss

Посмотрим на задержки, когда часть запросов обращаются по ключам, которых нет в кластере:

[GET-30k-key-miss.txt](GET-30k-key-miss.txt)

```
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency    21.50ms   49.34ms 260.48ms   87.17%
    Req/Sec     7.50k   226.62     8.31k    87.32%
  Latency Distribution (HdrHistogram - Recorded Latency)
 50.000%    1.75ms
 75.000%    2.86ms
 90.000%  110.46ms
 99.000%  203.39ms
 99.900%  234.49ms
 99.990%  252.16ms
 99.999%  259.58ms
100.000%  260.61ms
```

Latency увеличилась относительно [предыдщуего результата](../stage2/GET-30k-4threads-unlacky.txt).

Детальноно возможно это
связано с тем, что в текущем эксперименте больший объем базы данных.

### CPU profile

[GET-30k-cpu-key-miss.html](GET-30k-cpu-key-miss.html)

Профиль изменился - теперь больше сэмплов приходится на локальное чтение - теперь в ряде случаев (~30%) нам приходится 
обходить все sstable.

### Alloc profile

[GET-30k-alloc-key-miss.html](GET-30k-alloc-key-miss.html)

С точки зрения аллокаций изменений нет.

### Lock profile

[GET-30k-lock-key-miss.html](GET-30k-lock-key-miss.html)

На локах в целом всё тоже осталось по-прежнему. Единственное отличие - стало больше сэмплов в методе 
`Http1AsyncReciever::handlePendingDelegate`. Связано это может быть с тем, что запросы к нодам кластера теперь занимают 
больше времени (каждая нода выполняет трудоёмкий бинарный поиск по всем своим sstable) и нам приходится дольше ожидать 
ответ.
