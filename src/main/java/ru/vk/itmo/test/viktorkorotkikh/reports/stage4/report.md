# Stage 4

- [Stage 4](#stage-4)
    * [Конфигурация](#Конфигурация)
    * [PUT](#put)
        + [CPU profile](#cpu-profile)
        + [Alloc profile](#alloc-profile)
        + [Lock profile](#lock-profile)
    * [GET](#get)
        + [CPU profile](#cpu-profile-1)
        + [Alloc profile](#alloc-profile-1)
        + [Lock profile](#lock-profile-1)
    * [`OneNioHttpResponseWrapper` optimization](#OneNioHttpResponseWrapper-optimization)
        + [CPU profile](#cpu-profile-2)
        + [Alloc profile](#alloc-profile-2)
        + [Lock profile](#lock-profile-2)
    * [PUT-GET multi](#put-get-multi)
        + [Cpu profile](#cpu-profile-3)
        + [Alloc profile](#alloc-profile-3)
        + [Lock profile](#lock-profile-3)

## Конфигурация

wrk2 - 64 connections, 4 threads

Конфигурация кластера - 3 ноды, запущенные в отдельных процессах. Профилируем ноду, на которую шлём все запросы.

Запросы без параметров ack и from -> по умолчанию реплицирование 2 из 3

## PUT

[PUT-60k.txt](PUT-60k.txt)

```
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency    17.29s     7.29s   30.15s    57.82%
    Req/Sec     7.43k   102.97     7.58k    60.00%
  Latency Distribution (HdrHistogram - Recorded Latency)
 50.000%   17.19s 
 75.000%   23.58s 
 90.000%   27.51s 
 99.000%   29.87s 
 99.900%   30.10s 
 99.990%   30.13s 
 99.999%   30.15s 
100.000%   30.16s 
----------------------------------------------------------
  1790139 requests in 1.00m, 114.38MB read
Requests/sec:  29835.72
Transfer/sec:      1.91MB
```

Сразу видно, что точка разладки уехала вниз - нужно уменьшать нагрузку, 60k RPS мы уже не выдерживаем. Максимум теперь
30k RPS, это видно из отчёта wrk - больше пропихнуть он не смог.

Путём экспериментов была выбрана точка разладки в 25к RPS.

[PUT-25k.txt](PUT-25k.txt)

```
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency     1.76ms    1.18ms  16.37ms   89.30%
    Req/Sec     6.59k   562.16    10.00k    74.85%
  Latency Distribution (HdrHistogram - Recorded Latency)
 50.000%    1.54ms
 75.000%    2.08ms
 90.000%    2.68ms
 99.000%    7.21ms
 99.900%   11.73ms
 99.990%   14.84ms
 99.999%   15.73ms
100.000%   16.38ms
```

![PUT-25k-histogram.png](PUT-25k-histogram.png)

Очевидно, что точка разладки сместилась из-за того, что мы делаем обязательную пересылку по сети из-за фактора
реплицирования. Теперь у нас всегда идёт запись на все 3 ноды в кластере: локальная запись на пришедшую ноду и 2 
последовательных запроса по сети к двум другим нодам.

### CPU profile

[PUT-25k-cpu.html](PUT-25k-cpu.html)

На профиле стало видно работу GC - 6.26% от общего числа сэмплов. Полагаю, что это связано с тем, что раньше у нас были 
аллокации либо `MemorySegment` (и других связанных объектов, например, `ConcurrentSkipListMap$Node` или 
`FileOutputStream` при флаше на диск) при локальной записи, либо аллокации богатого внутреннего мира `HttpClient`. 
Теперь же у нас всегда производится локальная запись и 2 http запроса по сети. Также мы для каждого entry храним 
дополнительные 8 байт таймстемпа и пересылаем их по сети с дополнительным заголовком `X-Replica-Request`. Однако и 
нагрузка была в 2 раза больше, что, конечно, несколько смущает. Но снижение RPS обусловлено не затратами на аллокации, а 
последовательными запросами по сети. Вероятно, при асинхронном взаимодействии RPS для PUT запросов снова приблизится к 
отметке в 60k. Но это уже совсем другая история.

В остальном профиль в целом такой же, как и в [предыдущем stage](../stage3/PUT-60k-cpu.html).

Богатый внутренний мир HttpClient никуда не делся. Количество сэмплов на локальную запись уменьшилось до 1.76%, что
логично так как мы теперь ещё больше ходим по сети.

### Alloc profile

Аллокации тоже сильно не изменились (раньше было так [PUT-60k-alloc.html](../stage3/PUT-60k-alloc.html)):

[PUT-25k-alloc.html](PUT-25k-alloc.html)

### Lock profile

[PUT-25k-lock.html](PUT-25k-lock.html)

Блокировки также ([прошлый результат](../stage3/PUT-60k-lock.html)) в основном происходят
на `EPollSelectorImpl::wakeup`.

## GET

База объемом ~1.5G, каждая нода хранит около 517mb.

[GET-30k.txt](GET-30k.txt)

```
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency    23.30s     9.61s   40.01s    57.65%
    Req/Sec     2.50k     6.85     2.51k    75.00%
  Latency Distribution (HdrHistogram - Recorded Latency)
 50.000%   23.30s 
 75.000%   31.64s 
 90.000%   36.63s 
 99.000%   39.65s 
 99.900%   39.94s 
 99.990%   40.01s 
 99.999%   40.04s 
100.000%   40.04s 
----------------------------------------------------------
  599960 requests in 1.00m, 29.40MB read
  Non-2xx or 3xx responses: 17980
Requests/sec:   9999.36
Transfer/sec:    501.70KB
```

Ошибки Non-2xx or 3xx responses связаны с тем, что иногда мы ищем ключ, которого нет в нашем dao.

Показатели latency GET запросов тоже стали выше относительно [прошлого результата](../stage3/GET-30k.txt).
Да и явно точка разладки стала ниже.

Это связано с тем, что поиск по всем sstable осуществляется не на одной ноде, а на всех 3-х.

Максимум теперь 10k RPS, это видно из отчёта wrk - больше пропихнуть он не смог.

Путём экспериментов была выбрана точка разладки в 8к RPS.

[GET-8k.txt](GET-8k.txt)

```
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency     1.87ms    0.98ms  14.73ms   73.32%
    Req/Sec     2.11k   185.94     3.11k    62.94%
  Latency Distribution (HdrHistogram - Recorded Latency)
 50.000%    1.69ms
 75.000%    2.33ms
 90.000%    3.18ms
 99.000%    4.88ms
 99.900%    6.39ms
 99.990%   11.08ms
 99.999%   13.79ms
100.000%   14.73ms
```

![GET-8k-histogram.png](GET-8k-histogram.png)

### CPU profile

[GET-8k-cpu.html](GET-8k-cpu.html)

Теперь у нас локальное чтение занимает ~52% сэмплов, а общение по сети с другими нодами около ~33% (если объединить все
сэмплы, связанные с работой HttpClient, кроме работы с очередью, так как основной пул потоков использует ту же очередь).

Видим, что работа с локальными данными стала занимать почти в 2 раза больше сэмплов - это объясняется репликацией.
Раньше
запросы, которые принадлежат другим нодам, уходили только на них, а теперь ещё есть мы в 100% случаях из-за репликации 
2/3 (2 успешных ответа, но запрос обрабатывают все 3 ноды класетра) мы обрабатываем пришедший запрос и начинаем
искать значение по своим локальным sstable.

### Alloc profile

[GET-8k-alloc.html](GET-8k-alloc.html)

С точки зрения аллокаций тут всё снова примерно также, как и [было](../stage3/GET-30k-alloc.html).

Но есть и изменения - появились дополнительные аллокации в методе `mergeReplicasResponses`. Видно, что нужно изменить
подход к тому, как оборачивать ответы от нод из сети и локально. Сейчас `OneNioHttpResponseWrapper` занимает 10%
сэмплов.
Явно стоит пересмотреть данный подход.

### Lock profile

[GET-8k-lock.html](GET-8k-lock.html)

Локи изменились сильнее. Теперь стало ещё больше блокировок на очереди HttpClient так как по сети мы стали ходить чаще.

## `OneNioHttpResponseWrapper` optimization

Я решил заменить реализацию и добавил свой интерфейс `NodeResponse`. Посмотрим как изменились профили.

[GET-8k-2.txt](GET-8k-2.txt)

```
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency     1.77ms    0.94ms  10.38ms   72.44%
    Req/Sec     2.11k   183.01     3.00k    63.02%
  Latency Distribution (HdrHistogram - Recorded Latency)
 50.000%    1.60ms
 75.000%    2.23ms
 90.000%    3.04ms
 99.000%    4.67ms
 99.900%    6.16ms
 99.990%    8.32ms
 99.999%    9.66ms
100.000%   10.38ms
```

Показатели latency в последних перцентилях уменьшились.

### CPU profile

[GET-8k-cpu-2.html](GET-8k-cpu-2.html)

По профилю видно, что сравнение sstable было оптимизированно компилятором путём использования векторизированных 
инструкций. Скорее всего из-за этого и уменьшились задержки. В остальном профиль не поменялся.

На прошлых этапах эта оптимизация была. Сейчас получилось так, что в первом замере профиля GET не было этой оптимизации,
а в новом есть. Я думаю, что тут дело полностью в JIT и его хитростях. Первое, что приходит в голову - недостаточно была
прогрета JVM. Но я всегда перед снятием профилей прогреваю её запросами. Единственное, что могло повлиять в данном 
случае - это запуск GET нагрузки в первом случае сразу после PUT без перезагрузки. Причём PUT я делал достаточно долго, 
так как заполнил базу на 1.5gb. Возможно как раз недостаточно было запросов GET и JIT ещё не успел понять, что и как в 
данном методе можно оптимизировать. Либо просто ещё не до конца был вытеснен старый код, который JIT уже на PUT запросах
скомпилировал в нативный код. Во втором случае я PUT запросы заново не делал, а сразу прогревал GET-ами. Это могло 
сыграть свою роль.

В идеале нужно прогревать [мультинагрузкой](#put-get-multi) - и PUT и GET - так как логично, что пользователь не будет 
сначала заполнять базу, а потом только читать из неё. Такой профиль конечно возможен, но это редкость. Очевидно, что у 
нас и те, и другие запросы будут идти вместе (не факт, далеко не факт, что распределение 50/50, но тем не менее).

### Alloc profile

[GET-8k-alloc-2.html](GET-8k-alloc-2.html)

Вот теперь замечательно - аллокации в методе `mergeReplicasResponses` теперь занимают меньше одного процента сэмплов.

### Lock profile

[GET-8k-lock-2.html](GET-8k-lock-2.html)

На локах всё тоже по-прежнему. По-другому быть и не могло, так как никаких изменений в работе блокирующего кода не было.

## PUT-GET multi

Луа скрипт случайным образом выбирает PUT или GET запрос, который будет отправлять. Размер базы на момент начала 
тестирования ~1.92Gb суммарно.

[stage-4-PUT-GET.lua](../../scripts/stage-4-PUT-GET.lua)

Так как GET запросы медленнее, чем PUT запросы, то было решено выбрать уровень нагрузки GET запросов в 8k RPS.

По результатам видно, что задержки примерно на уровне GET запросов без PUT:
```
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency     1.42ms  790.96us  16.48ms   75.98%
    Req/Sec     2.11k   228.34     3.33k    79.47%
  Latency Distribution (HdrHistogram - Recorded Latency)
 50.000%    1.29ms
 75.000%    1.77ms
 90.000%    2.34ms
 99.000%    4.27ms
 99.900%    6.42ms
 99.990%    9.78ms
 99.999%   12.44ms
100.000%   16.50ms
```

[PUT-GET-8k.txt](PUT-GET-8k.txt)

Ошибки при выполнении запросов имеют 2 причины:
1. Попытка записать в полную memtable при активном фоновом флаше
2. GET по несуществующему ключу

### Cpu profile

[PUT-GET-8k.html](PUT-GET-8k.html)

На профиле видно, что JIT оптимизировал mismatch с помощью использования векторных инструкций. Также отчётливо видно, что
GET запросы тяжелее - PUT меньше 1% сэмплов. Это так, потому что на GET нам приходится локально прочёсывать в худшем 
случае все ssatble.

### Alloc profile

[PUT-GET-8k-alloc.html](PUT-GET-8k-alloc.html)

С точки зрения аллокаций профиль похож на нечто среднее между PUT и GET профилями в отдельности.

### Lock profile

[PUT-GET-8k-lock.html](PUT-GET-8k-lock.html)

А вот профиль локов похож на профиль GET запросов. Стало больше сэмплов `handlePendingDelegate` - это эффект PUT 
запросов (на профиле PUT их тоже больше). Вероятно, это связано с отправкой тела запроса - `HttpClient` регистрирует 
событие подписки на завершение отправки запроса в сокет, но пока сокет занят, то поток заблокирован; 
`EPollSelectorImpl::wakeup` вызывается для пробуждения (метод `synchronized`, поэтому локи).
