# Stage 5

- [Stage 5](#stage-5)
    * [Конфигурация](#Конфигурация)
    * [PUT](#put)
        + [CPU profile](#cpu-profile)
        + [Alloc profile](#alloc-profile)
        + [Lock profile](#lock-profile)
    * [GET](#get)
        + [CPU profile](#cpu-profile-1)
        + [Alloc profile](#alloc-profile-1)
        + [Lock profile](#lock-profile-1)
    * [`OneNioHttpResponseWrapper` optimization](#OneNioHttpResponseWrapper-optimization)
        + [CPU profile](#cpu-profile-2)
        + [Alloc profile](#alloc-profile-2)
        + [Lock profile](#lock-profile-2)
    * [PUT-GET multi](#put-get-multi)
        + [Cpu profile](#cpu-profile-3)
        + [Alloc profile](#alloc-profile-3)
        + [Lock profile](#lock-profile-3)

## Конфигурация

wrk2 - 64 connections, 4 threads

Конфигурация кластера - 3 ноды, запущенные в отдельных процессах. Профилируем ноду, на которую шлём все запросы.

Запросы без параметров ack и from -> по умолчанию реплицирование 2 из 3

## PUT

[PUT-60k.txt](PUT-60k.txt)

```
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency    12.97s     5.67s   23.43s    58.22%
    Req/Sec     9.02k   283.19     9.27k    87.50%
  Latency Distribution (HdrHistogram - Recorded Latency)
 50.000%   12.80s 
 75.000%   17.84s 
 90.000%   20.92s 
 99.000%   23.17s 
 99.900%   23.38s 
 99.990%   23.41s 
 99.999%   23.43s 
100.000%   23.45s
----------------------------------------------------------
  2194003 requests in 1.00m, 140.19MB read
  Non-2xx or 3xx responses: 1
Requests/sec:  36566.85
Transfer/sec:      2.34MB 
```

60k RPS мы всё ещё не держим, но показатели лучше, чем в [прошлой реализации](../stage4/PUT-60k.txt) - wrk удалось
пропихнуть на 6.5k RPS больше. Посмотрим, что происходит на нагрузке в 25k RPS:

[PUT-25k.txt](PUT-25k.txt)

```
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency     1.35ms  609.01us   8.03ms   70.39%
    Req/Sec     6.25k     8.63     6.28k    76.12%
  Latency Distribution (HdrHistogram - Recorded Latency)
 50.000%    1.27ms
 75.000%    1.70ms
 90.000%    2.13ms
 99.000%    3.20ms
 99.900%    4.40ms
 99.990%    5.68ms
 99.999%    7.04ms
100.000%    8.03ms
```

![PUT-25k-histogram.png](PUT-25k-histogram.png)

Latency на нагрузке в 25k RPS уменьшились в 2 раза, а график теперь похож на прямую зависимость.
Имеет смысл найти новую точку разладки. Очевидно, что это где-то между 25k и 36.5k (столько максимум смог впихнуть wrk
при требуемой от него нагрузки в 60k RPS).

Путём экспериментов была выбрана точка разладки в 31.5к RPS.

[PUT-31.5k.txt](PUT-31.5k.txt)

```
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency     1.79ms    0.88ms  12.33ms   77.46%
    Req/Sec     8.30k   813.43    12.00k    72.55%
  Latency Distribution (HdrHistogram - Recorded Latency)
 50.000%    1.63ms
 75.000%    2.19ms
 90.000%    2.82ms
 99.000%    4.61ms
 99.900%    8.77ms
 99.990%   11.05ms
 99.999%   11.90ms
100.000%   12.34ms
```

![PUT-31.5k-histogram.png](PUT-31.5k-histogram.png)

Благодаря асинхронным операциям удалось нарастить пропускную способность нашего кластера на 26%. Теперь мы по-настоящему
ждём 2 успешных ответа из 3 и после этого отвечаем на запрос, соответственно, 1/3 самых медленных запросов выполняется,
но мы не ждём их полного завершения. Это в целом коррелирует с приростом в 26%.

### CPU profile

[PUT-31k-cpu.html](PUT-31k-cpu.html)

На профиле также, как и в [прошлый раз](../stage4/report.md#cpu-profile) видно работу GC - 6% от общего числа сэмплов.

Уменьшилось количество сэмплов метода `handleEntityRequest` по сравнению
с [предыдущим stage](../stage4/PUT-25k-cpu.html):

1. В 2 раза меньше сэмплов в методе `processRemote`, так как теперь мы обрабатываем ответы в
   отдельном `ExecutorService`.
2. `LsmCustomSession::sendResponse` выполняется тоже асинхронно в отдельном `ExecutorService`.

В остальном профиль в целом такой же.

### Alloc profile

Аллокации не изменились (в рамках погрешности) (раньше было так [PUT-25k-alloc.html](../stage4/PUT-25k-alloc.html)):

[PUT-31k-alloc.html](PUT-31k-alloc.html)

### Lock profile

[PUT-31k-lock.html](PUT-31k-lock.html)

Блокировки относительно [прошлого результата](../stage4/PUT-25k-lock.html) изменились:

1. Появились явные блокировки на `HttpClient` при вызове метода `sendAsync`.
2. Блокировки на методе `HttpClient::cancelTimer` из-за того, что таймаут теперь реализуется не с помощью
   метода `CompletableFututre::get`, а при создании запроса через builder.
3. Блокировки на внутренних локах `HttpClient`
4. Почти исчезли блокировки на `EPollSelectorImpl::wakeup` - вероятно из-за лучшей утилизации пула потоков, который мы
   передали в `HttpClient`, ведь теперь мы не последовательно выполняем 1, а потом второй запрос (в рамках одного
   пришедшего запроса), а 2 запроса параллельно, что, вероятно, позволяет потокам не ждать события сокета, а заниматься 
   обработкой других запросов.

## GET

// TODO

База объемом ~1.5G, каждая нода хранит около 517mb.

[GET-30k.txt](GET-30k.txt)

```
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency    23.30s     9.61s   40.01s    57.65%
    Req/Sec     2.50k     6.85     2.51k    75.00%
  Latency Distribution (HdrHistogram - Recorded Latency)
 50.000%   23.30s 
 75.000%   31.64s 
 90.000%   36.63s 
 99.000%   39.65s 
 99.900%   39.94s 
 99.990%   40.01s 
 99.999%   40.04s 
100.000%   40.04s 
----------------------------------------------------------
  599960 requests in 1.00m, 29.40MB read
  Non-2xx or 3xx responses: 17980
Requests/sec:   9999.36
Transfer/sec:    501.70KB
```

Ошибки Non-2xx or 3xx responses связаны с тем, что иногда мы ищем ключ, которого нет в нашем dao.

Показатели latency GET запросов тоже стали выше относительно [прошлого результата](../stage3/GET-30k.txt).
Да и явно точка разладки стала ниже.

Это связано с тем, что поиск по всем sstable осуществляется не на одной ноде, а на всех 3-х.

Максимум теперь 10k RPS, это видно из отчёта wrk - больше пропихнуть он не смог.

Путём экспериментов была выбрана точка разладки в 8к RPS.

[GET-8k.txt](GET-8k.txt)

```
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency     1.87ms    0.98ms  14.73ms   73.32%
    Req/Sec     2.11k   185.94     3.11k    62.94%
  Latency Distribution (HdrHistogram - Recorded Latency)
 50.000%    1.69ms
 75.000%    2.33ms
 90.000%    3.18ms
 99.000%    4.88ms
 99.900%    6.39ms
 99.990%   11.08ms
 99.999%   13.79ms
100.000%   14.73ms
```

![GET-8k-histogram.png](GET-8k-histogram.png)

### CPU profile

[GET-8k-cpu.html](GET-8k-cpu.html)

Теперь у нас локальное чтение занимает ~52% сэмплов, а общение по сети с другими нодами около ~33% (если объединить все
сэмплы, связанные с работой HttpClient, кроме работы с очередью, так как основной пул потоков использует ту же очередь).

Видим, что работа с локальными данными стала занимать почти в 2 раза больше сэмплов - это объясняется репликацией.
Раньше
запросы, которые принадлежат другим нодам, уходили только на них, а теперь ещё есть мы в 100% случаях из-за репликации
2/3 (2 успешных ответа, но запрос обрабатывают все 3 ноды класетра) мы обрабатываем пришедший запрос и начинаем
искать значение по своим локальным sstable.

### Alloc profile

[GET-8k-alloc.html](GET-8k-alloc.html)

С точки зрения аллокаций тут всё снова примерно также, как и [было](../stage3/GET-30k-alloc.html).

Но есть и изменения - появились дополнительные аллокации в методе `mergeReplicasResponses`. Видно, что нужно изменить
подход к тому, как оборачивать ответы от нод из сети и локально. Сейчас `OneNioHttpResponseWrapper` занимает 10%
сэмплов.
Явно стоит пересмотреть данный подход.

### Lock profile

[GET-8k-lock.html](GET-8k-lock.html)

Локи изменились сильнее. Теперь стало ещё больше блокировок на очереди HttpClient так как по сети мы стали ходить чаще.

## `OneNioHttpResponseWrapper` optimization

Я решил заменить реализацию и добавил свой интерфейс `NodeResponse`. Посмотрим как изменились профили.

[GET-8k-2.txt](GET-8k-2.txt)

```
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency     1.77ms    0.94ms  10.38ms   72.44%
    Req/Sec     2.11k   183.01     3.00k    63.02%
  Latency Distribution (HdrHistogram - Recorded Latency)
 50.000%    1.60ms
 75.000%    2.23ms
 90.000%    3.04ms
 99.000%    4.67ms
 99.900%    6.16ms
 99.990%    8.32ms
 99.999%    9.66ms
100.000%   10.38ms
```

Показатели latency в последних перцентилях уменьшились.

### CPU profile

[GET-8k-cpu-2.html](GET-8k-cpu-2.html)

По профилю видно, что сравнение sstable было оптимизированно компилятором путём использования векторизированных
инструкций. Скорее всего из-за этого и уменьшились задержки. В остальном профиль не поменялся.

На прошлых этапах эта оптимизация была. Сейчас получилось так, что в первом замере профиля GET не было этой оптимизации,
а в новом есть. Я думаю, что тут дело полностью в JIT и его хитростях. Первое, что приходит в голову - недостаточно была
прогрета JVM. Но я всегда перед снятием профилей прогреваю её запросами. Единственное, что могло повлиять в данном
случае - это запуск GET нагрузки в первом случае сразу после PUT без перезагрузки. Причём PUT я делал достаточно долго,
так как заполнил базу на 1.5gb. Возможно как раз недостаточно было запросов GET и JIT ещё не успел понять, что и как в
данном методе можно оптимизировать. Либо просто ещё не до конца был вытеснен старый код, который JIT уже на PUT запросах
скомпилировал в нативный код. Во втором случае я PUT запросы заново не делал, а сразу прогревал GET-ами. Это могло
сыграть свою роль.

В идеале нужно прогревать [мультинагрузкой](#put-get-multi) - и PUT и GET - так как логично, что пользователь не будет
сначала заполнять базу, а потом только читать из неё. Такой профиль конечно возможен, но это редкость. Очевидно, что у
нас и те, и другие запросы будут идти вместе (не факт, далеко не факт, что распределение 50/50, но тем не менее).

### Alloc profile

[GET-8k-alloc-2.html](GET-8k-alloc-2.html)

Вот теперь замечательно - аллокации в методе `mergeReplicasResponses` теперь занимают меньше одного процента сэмплов.

### Lock profile

[GET-8k-lock-2.html](GET-8k-lock-2.html)

На локах всё тоже по-прежнему. По-другому быть и не могло, так как никаких изменений в работе блокирующего кода не было.

## PUT-GET multi

Луа скрипт случайным образом выбирает PUT или GET запрос, который будет отправлять. Размер базы на момент начала
тестирования ~1.92Gb суммарно.

[stage-4-PUT-GET.lua](../../scripts/stage-4-PUT-GET.lua)

Так как GET запросы медленнее, чем PUT запросы, то было решено выбрать уровень нагрузки GET запросов в 8k RPS.

По результатам видно, что задержки примерно на уровне GET запросов без PUT:

```
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency     1.42ms  790.96us  16.48ms   75.98%
    Req/Sec     2.11k   228.34     3.33k    79.47%
  Latency Distribution (HdrHistogram - Recorded Latency)
 50.000%    1.29ms
 75.000%    1.77ms
 90.000%    2.34ms
 99.000%    4.27ms
 99.900%    6.42ms
 99.990%    9.78ms
 99.999%   12.44ms
100.000%   16.50ms
```

[PUT-GET-8k.txt](PUT-GET-8k.txt)

Ошибки при выполнении запросов имеют 2 причины:

1. Попытка записать в полную memtable при активном фоновом флаше
2. GET по несуществующему ключу

### Cpu profile

[PUT-GET-8k.html](PUT-GET-8k.html)

На профиле видно, что JIT оптимизировал mismatch с помощью использования векторных инструкций. Также отчётливо видно,
что
GET запросы тяжелее - PUT меньше 1% сэмплов. Это так, потому что на GET нам приходится локально прочёсывать в худшем
случае все ssatble.

### Alloc profile

[PUT-GET-8k-alloc.html](PUT-GET-8k-alloc.html)

С точки зрения аллокаций профиль похож на нечто среднее между PUT и GET профилями в отдельности.

### Lock profile

[PUT-GET-8k-lock.html](PUT-GET-8k-lock.html)

А вот профиль локов похож на профиль GET запросов. Стало больше сэмплов `handlePendingDelegate` - это эффект PUT
запросов (на профиле PUT их тоже больше). Вероятно, это связано с отправкой тела запроса - `HttpClient` регистрирует
событие подписки на завершение отправки запроса в сокет, но пока сокет занят, то поток заблокирован;
`EPollSelectorImpl::wakeup` вызывается для пробуждения (метод `synchronized`, поэтому локи).
